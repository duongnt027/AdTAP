{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14052957,"sourceType":"datasetVersion","datasetId":8727126}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport random\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, roc_auc_score\nfrom torchvision import models\nimport torch.optim as optim","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-08T19:49:11.055437Z","iopub.execute_input":"2025-12-08T19:49:11.056133Z","iopub.status.idle":"2025-12-08T19:49:21.056672Z","shell.execute_reply.started":"2025-12-08T19:49:11.056107Z","shell.execute_reply":"2025-12-08T19:49:21.055847Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class ContrastiveEnv:\n    def __init__(self, X=None, Y=None, device=\"cpu\"):\n        self.device = device\n        self.set_data(X, Y)\n\n    def set_data(self, X, Y):\n        if X is None or Y is None:\n            # Chưa có dữ liệu → để trống\n            self.X = None\n            self.Y = None\n            self.N = 0\n        else:\n            # Kiểm tra xem X và Y đã là tensor chưa\n            if isinstance(X, torch.Tensor):\n                self.X = X.detach().clone().to(self.device)\n            else:\n                self.X = torch.tensor(X, dtype=torch.float32).to(self.device)\n            \n            if isinstance(Y, torch.Tensor):\n                self.Y = Y.detach().clone().to(self.device)\n            else:\n                self.Y = torch.tensor(Y, dtype=torch.float32).to(self.device)\n            \n            self.N = len(self.X)\n\n    def reset(self, idx):\n        \"\"\"Lấy state = [x, y] tại index idx\"\"\"\n        x = self.X[idx]\n        y = self.Y[idx]\n        state = torch.cat([x, y])      # size = 65\n        return state\n\n    def step(self, idx, mu, logvar):\n        \"\"\"Nhận action (mu, logvar) → sample x' → compute reward\"\"\"\n        eps = torch.randn_like(mu)\n        x_prime = mu + torch.exp(0.5 * logvar) * eps\n\n        x = self.X[idx]\n        y = self.Y[idx]\n\n        # ---------- similarity ----------\n        cos_sim = torch.nn.functional.cosine_similarity(x, x_prime, dim=0)\n        sim_reward = 1 - torch.sigmoid(1 - cos_sim)\n\n        # ---------- contrastive ----------\n        same_cls = self.X[(self.Y == y).squeeze()]\n        diff_cls = self.X[(self.Y != y).squeeze()]\n\n        mean_same = torch.mean(torch.norm(x_prime - same_cls, dim=1))\n        mean_diff = torch.mean(torch.norm(x_prime - diff_cls, dim=1))\n\n        contrastive_penalty = mean_same - mean_diff\n\n        reward = sim_reward - contrastive_penalty\n        reward = reward.unsqueeze(0)\n\n        next_state = torch.cat([x, y])\n        done = True  # 1-step environment\n        \n        return next_state, reward, done\n\nclass Actor(nn.Module):\n    def __init__(self, input_dim=64, hidden_dim=128, z_dim=64):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim+1, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU()\n        )\n        self.mu = nn.Linear(hidden_dim, z_dim)\n        self.logvar = nn.Linear(hidden_dim, z_dim)\n\n    def forward(self, state):\n        h = self.net(state)\n        return self.mu(h), self.logvar(h)\n\nclass Critic(nn.Module):\n    def __init__(self, input_dim=64, hidden_dim=128):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim+1, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n\n    def forward(self, state):\n        return self.net(state)\n\nclass PPO:\n    def __init__(\n            self,\n            actor,\n            critic,\n            env,\n            lr=3e-4,\n            gamma=0.99,\n            clip_eps=0.2,\n            device=\"cpu\"\n        ):\n        self.actor = actor.to(device)\n        self.critic = critic.to(device)\n        self.env = env\n        self.gamma = gamma\n        self.clip_eps = clip_eps\n        self.device = device\n\n        self.optimizer_actor = optim.Adam(actor.parameters(), lr=lr)\n        self.optimizer_critic = optim.Adam(critic.parameters(), lr=lr)\n\n    def infer(self, state):\n        \"\"\"Lấy action từ policy\"\"\"\n        state = state.to(self.device)\n        mu, logvar = self.actor(state)\n        eps = torch.randn_like(mu)\n        new_state = mu + torch.exp(0.5 * logvar) * eps\n        return new_state\n\n    def learn(self, batch_indices, epochs=5):\n        \"\"\"Huấn luyện PPO trên batch index\"\"\"\n\n        states = []\n        actions = []\n        rewards = []\n        values = []\n        log_probs = []\n        next_states = []\n\n        with torch.no_grad():  # Không cần gradient khi collect data\n            for idx in batch_indices:\n                state = self.env.reset(idx)\n                mu, logvar = self.actor(state)\n\n                # Sample x'\n                eps = torch.randn_like(mu)\n                action = mu + torch.exp(0.5 * logvar) * eps\n\n                # log probability\n                logp = -0.5 * ((action - mu) ** 2 / torch.exp(logvar) + logvar)\n                logp = logp.sum()\n\n                value = self.critic(state)\n\n                next_state, reward, done = self.env.step(idx, mu, logvar)\n\n                states.append(state)\n                actions.append(action)\n                rewards.append(reward)\n                values.append(value)\n                log_probs.append(logp)\n                next_states.append(next_state)\n\n        # Convert to tensors\n        states = torch.stack(states).detach()\n        actions = torch.stack(actions).detach()\n        rewards = torch.stack(rewards).detach()\n        values = torch.stack(values).detach()\n        old_log_probs = torch.stack(log_probs).detach()\n\n        # Advantage\n        returns = rewards\n        advantages = (returns - values).detach()  # Detach advantages\n\n        for epoch in range(epochs):\n            # Recompute log probs và values mỗi epoch\n            mu, logvar = self.actor(states)\n            eps = (actions - mu) / (torch.exp(0.5 * logvar) + 1e-8)  # Thêm epsilon để tránh chia cho 0\n            logp = -0.5 * ((eps ** 2) + logvar)\n            logp = logp.sum(dim=1)\n\n            ratio = torch.exp(logp - old_log_probs)\n\n            surr1 = ratio * advantages.squeeze()\n            surr2 = torch.clamp(ratio, 1 - self.clip_eps, 1 + self.clip_eps) * advantages.squeeze()\n\n            actor_loss = -torch.min(surr1, surr2).mean()\n            \n            # Update actor\n            self.optimizer_actor.zero_grad()\n            actor_loss.backward()\n            self.optimizer_actor.step()\n\n            # Recompute values cho critic loss\n            current_values = self.critic(states).squeeze()\n            critic_loss = nn.MSELoss()(current_values, returns.squeeze())\n            \n            # Update critic\n            self.optimizer_critic.zero_grad()\n            critic_loss.backward()\n            self.optimizer_critic.step()\n\n        avg_reward = rewards.mean().item()\n\n        return actor_loss.item(), critic_loss.item(), avg_reward","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T19:49:21.057875Z","iopub.execute_input":"2025-12-08T19:49:21.058263Z","iopub.status.idle":"2025-12-08T19:49:21.078512Z","shell.execute_reply.started":"2025-12-08T19:49:21.058238Z","shell.execute_reply":"2025-12-08T19:49:21.077743Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class NPZDataset(Dataset):\n    def __init__(self, path, indices=None, resize=256, device=\"cpu\"):\n        self.data = np.load(path, allow_pickle=True, mmap_mode=\"r\")\n        self.images = self.data[\"images\"]\n        self.labels = self.data[\"labels\"]\n\n        if indices is None:\n            self.indices = np.arange(len(self.labels))\n        else:\n            self.indices = indices\n\n        self.resize = resize\n        self.device = device\n\n    def __len__(self):\n        return len(self.indices)\n\n    def __getitem__(self, idx):\n        i = self.indices[idx]\n        img = torch.tensor(self.images[i]).float()\n        lbl = torch.tensor(self.labels[i]).float()\n\n        img = img.permute(2,0,1)\n        if img.max() > 1:\n            img = img / 255.\n\n        img = F.interpolate(\n            img.unsqueeze(0),\n            size=(self.resize, self.resize),\n            mode=\"bilinear\",\n            align_corners=False\n        ).squeeze(0)\n\n        return img.to(self.device), lbl.to(self.device)\n\nclass SyntheticDataset(Dataset):\n    def __init__(self, path, device):\n        d = np.load(path)\n        self.images = d[\"images\"]\n        self.labels = d[\"labels\"]\n        self.device = device\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        img = torch.tensor(self.images[idx], dtype=torch.float32).permute(2,0,1)\n        lbl = torch.tensor(self.labels[idx], dtype=torch.float32)\n        return img.to(self.device), lbl.to(self.device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T19:49:21.079317Z","iopub.execute_input":"2025-12-08T19:49:21.079579Z","iopub.status.idle":"2025-12-08T19:49:21.107497Z","shell.execute_reply.started":"2025-12-08T19:49:21.079553Z","shell.execute_reply":"2025-12-08T19:49:21.106947Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def load_train_test(path, device=\"cpu\", batch_size=32, resize=256, test_size=0.6):\n    dataset_full = NPZDataset(path, device=device, resize=resize)\n    indices = np.arange(len(dataset_full.labels))\n\n    # Nếu test_size = 0 → không split\n    if test_size == 0:\n        train_idx = indices\n        test_idx = []\n    else:\n        train_idx, test_idx = train_test_split(\n            indices,\n            test_size=test_size,\n            stratify=dataset_full.labels,\n            random_state=42\n        )\n\n    train_dataset = NPZDataset(path, indices=train_idx, device=device, resize=resize)\n    test_dataset  = NPZDataset(path, indices=test_idx,  device=device, resize=resize) if len(test_idx) > 0 else None\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader  = DataLoader(test_dataset,  batch_size=batch_size) if test_dataset is not None else None\n\n    return train_dataset, train_loader, test_dataset, test_loader\n\ndef count_classes(dataset):\n    count0 = 0\n    count1 = 0\n\n    for i in range(len(dataset)):\n        lbl = dataset[i][1]    # dataset[i] → (image, label)\n        lbl = int(lbl.item())  # convert to 0 hoặc 1\n\n        if lbl == 0:\n            count0 += 1\n        else:\n            count1 += 1\n    return count0, count1\n\ndef evaluate_model(model, loader, device, threshold=0.5):\n    model.eval()\n    all_preds, all_labels = [], []\n\n    with torch.no_grad():\n        for x_batch, y_batch in loader:\n            x_batch = x_batch.to(device)\n            y_batch = y_batch.to(device)\n            y_pred = model(x_batch).squeeze()\n            all_preds.append(y_pred.cpu())\n            all_labels.append(y_batch.cpu())\n\n    preds = torch.cat(all_preds).numpy()\n    labels = torch.cat(all_labels).numpy()\n\n\n    binary_pred = (preds > threshold).astype(int)\n\n    print(\"Classification Report:\")\n    print(classification_report(labels, binary_pred, labels=[0,1], target_names=[\"Class 0\",\"Class 1\"]))\n\n    if len(set(labels)) > 1:\n        auc = roc_auc_score(labels, preds)\n        print(f\"AUC-ROC: {auc:.4f}\")\n    else:\n        print(\"AUC-ROC: Undefined (only one class present)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T19:49:21.109349Z","iopub.execute_input":"2025-12-08T19:49:21.109748Z","iopub.status.idle":"2025-12-08T19:49:21.132029Z","shell.execute_reply.started":"2025-12-08T19:49:21.109729Z","shell.execute_reply":"2025-12-08T19:49:21.131318Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class ResNetEmbedder(nn.Module):\n    def __init__(self, output_dim=128, pretrained=True):\n        super().__init__()\n        self.backbone = models.resnet50(pretrained=pretrained)\n        self.backbone.fc = nn.Identity()\n        self.dim_adjuster = nn.Linear(2048, output_dim)\n        self.output_dim = output_dim\n\n    def forward(self, images):\n        features = self.backbone(images)\n        features = features.squeeze()\n        if features.dim() == 1:\n            features = features.unsqueeze(0)\n        features = self.dim_adjuster(features)\n        return features\n\nclass CNNAutoEncoder(nn.Module):\n    def __init__(self, latent_dim=128):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            nn.Conv2d(3,64,4,2,1), nn.BatchNorm2d(64), nn.ReLU(),\n            nn.Conv2d(64,128,4,2,1), nn.BatchNorm2d(128), nn.ReLU(),\n            nn.Conv2d(128,256,4,2,1), nn.BatchNorm2d(256), nn.ReLU(),\n            nn.Conv2d(256,512,4,2,1), nn.BatchNorm2d(512), nn.ReLU()\n        )\n        self.encoder2latent = nn.Linear(512*16*16+1, latent_dim)\n        self.latent2decoder = nn.Linear(latent_dim+1, 512*16*16)\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(512,256,4,2,1), nn.BatchNorm2d(256), nn.ReLU(),\n            nn.ConvTranspose2d(256,128,4,2,1), nn.BatchNorm2d(128), nn.ReLU(),\n            nn.ConvTranspose2d(128,64,4,2,1), nn.BatchNorm2d(64), nn.ReLU(),\n            nn.ConvTranspose2d(64,3,4,2,1), nn.Sigmoid()\n        )\n\n    def encode(self, x, y):\n        h = self.encoder(x)\n        flat = h.view(x.size(0), -1)\n        xy = torch.cat([flat, y], dim=1)\n        z = self.encoder2latent(xy)\n        return z\n\n    def decode(self, z, y):\n        zy = torch.cat([z, y], dim=1)\n        flat = self.latent2decoder(zy)\n        h = flat.view(z.size(0),512,16,16)\n        x_recon = self.decoder(h)\n        return x_recon\n\n    def forward(self, x, y):\n        z = self.encode(x, y)\n        return self.decode(z, y)\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0,max_len).unsqueeze(1).float()\n        div_term = torch.exp(torch.arange(0,d_model,2).float()*(-np.log(10000.0)/d_model))\n        pe[:,0::2] = torch.sin(position*div_term)\n        pe[:,1::2] = torch.cos(position*div_term)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self,x):\n        L = x.size(1)\n        return x + self.pe[:,:L,:]\n\nclass ViTransformerDetector(nn.Module):\n    def __init__(self, embedder, d_model=128, nhead=8, num_layers=2, dim_feedforward=256, dropout=0.1):\n        super().__init__()\n        self.embedder = embedder\n        self.embedding_dim = embedder.output_dim\n        self.projection = nn.Linear(self.embedding_dim,d_model)\n        self.positional_encoding = PositionalEncoding(d_model)\n        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,\n                                                   dim_feedforward=dim_feedforward,\n                                                   dropout=dropout, batch_first=True)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.fc = nn.Sequential(\n            nn.Linear(d_model,128), nn.ReLU(), nn.Dropout(dropout),\n            nn.Linear(128,1), nn.Sigmoid()\n        )\n\n    def forward(self,x):\n        emb = self.embedder(x).view(x.size(0),1,-1)\n        emb = self.projection(emb)\n        emb = self.positional_encoding(emb)\n        h = self.transformer_encoder(emb)\n        pooled = h.mean(dim=1)\n        return self.fc(pooled).squeeze(1)\n\nclass EarlyStopping:\n    def __init__(self, patience=5, min_delta=1e-4):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.best_loss = float(\"inf\")\n        self.early_stop = False\n\n    def step(self, current_loss):\n        if current_loss < self.best_loss - self.min_delta:\n            self.best_loss = current_loss\n            self.counter = 0\n        else:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True\n\n    def reset(self):\n        self.counter = 0\n        self.best_loss = float(\"inf\")\n        self.early_stop = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T19:49:21.132629Z","iopub.execute_input":"2025-12-08T19:49:21.132804Z","iopub.status.idle":"2025-12-08T19:49:21.149219Z","shell.execute_reply.started":"2025-12-08T19:49:21.132790Z","shell.execute_reply":"2025-12-08T19:49:21.148507Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def train_cnn_ae(model, loader, opt, device):\n    model.train()\n    total = 0\n    for x,y in loader:\n        x = x.to(device)\n        y = y.unsqueeze(1).to(device)\n        recon = model.decode(model.encode(x,y), y)\n        loss = F.mse_loss(recon, x)\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n        total += loss.item()\n    return total/len(loader)\n\ndef train_detector(model, loader, opt, criterion, device):\n    model.train()\n    total = 0\n    for x,y in loader:\n        x=x.to(device); y=y.to(device)\n        out = model(x)\n        loss = criterion(out, y)\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n        total += loss.item()\n    return total/len(loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T19:49:21.150548Z","iopub.execute_input":"2025-12-08T19:49:21.150866Z","iopub.status.idle":"2025-12-08T19:49:21.169591Z","shell.execute_reply.started":"2025-12-08T19:49:21.150848Z","shell.execute_reply":"2025-12-08T19:49:21.168874Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def One_Step_To_Feasible_Action(generator, detector, x_orig, device,\n                                previously_generated=None, alpha=1.0,\n                                lambda_div=0.1, lr=0.001, steps=50):\n    generator.eval()\n    detector.eval()\n    if previously_generated is None:\n        previously_generated = []\n\n    x_orig = x_orig.to(device).unsqueeze(0)\n    y_class1 = torch.full((1,1),0.8,device=device)\n\n    with torch.no_grad():\n        z = generator.encode(x_orig, y_class1)\n    z = z.clone().detach().requires_grad_(True)\n\n    optimizer_z = torch.optim.Adam([z], lr=lr)\n\n    for step in range(steps):\n        optimizer_z.zero_grad()\n        x_syn = generator.decode(z, y_class1)\n        prob_class1 = detector(x_syn)\n\n        if len(previously_generated) > 0:\n            old = torch.stack(previously_generated).to(device)\n            diff = x_syn.unsqueeze(1)-old.unsqueeze(0)\n            dist_min = diff.norm(p=2,dim=(2,3,4)).min(dim=1).values\n            diversity_term = torch.exp(-alpha*dist_min).mean()\n        else:\n            diversity_term = torch.tensor(0.0, device=device)\n\n        reward = prob_class1.mean() + lambda_div*diversity_term\n        reward.backward()\n        optimizer_z.step()\n\n    with torch.no_grad():\n        x_adv = generator.decode(z, y_class1).detach().cpu()\n\n    return x_adv\n\ndef Gen_with_PPO(ppo, generator, x_orig, device):\n    generator.eval()\n\n    x_orig = x_orig.to(device).unsqueeze(0)\n    y_class1 = torch.full((1,1), random.uniform(0.8, 1.0), device=device)\n\n    with torch.no_grad():\n        z = generator.encode(x_orig, y_class1)\n    \n    state = torch.cat([z, y_class1], dim=1)\n    z_adv = ppo.infer(state)\n    \n    # Kiểm tra valid variant\n    if check_valid_variant(z_adv, z):\n        with torch.no_grad():\n            x_synthetic = generator.decode(z_adv, y_class1)\n        return x_synthetic\n    else:\n        return None\n\n\ndef check_valid_variant(x, x_prime, threshold=0.3):\n    \"\"\"\n    Kiểm tra xem x_prime có phải là variant hợp lệ của x không\n    Returns: bool (True nếu valid)\n    \"\"\"\n    # Flatten nếu có batch dimension\n    if x.dim() > 1:\n        x_flat = x.view(x.size(0), -1)\n        x_prime_flat = x_prime.view(x_prime.size(0), -1)\n        cos_sim = torch.nn.functional.cosine_similarity(x_flat, x_prime_flat, dim=1)\n    else:\n        cos_sim = torch.nn.functional.cosine_similarity(x, x_prime, dim=0)\n    \n    cos_dist = 1 - cos_sim\n    \n    # Trả về boolean value duy nhất\n    # Nếu có nhiều samples, kiểm tra xem TẤT CẢ có valid không\n    is_valid = (cos_dist < threshold).all()\n    \n    return is_valid.item()  # Convert tensor to Python bool\n\ndef load_z_space(generator, loader, device):\n    generator.eval()\n\n    all_z = []\n    all_y = []\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device)\n            y = y.unsqueeze(1).to(device)\n\n            z = generator.encode(x, y)   # (batch, z_dim)\n\n            all_z.append(z.cpu())\n            all_y.append(y.cpu())\n\n    # Ghép toàn bộ batch lại\n    all_z = torch.cat(all_z, dim=0)\n    all_y = torch.cat(all_y, dim=0)\n\n    return all_z, all_y\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T19:49:21.170276Z","iopub.execute_input":"2025-12-08T19:49:21.170453Z","iopub.status.idle":"2025-12-08T19:49:21.193414Z","shell.execute_reply.started":"2025-12-08T19:49:21.170439Z","shell.execute_reply":"2025-12-08T19:49:21.192724Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nDATASET_DIR = \"/kaggle/input/bmad-80-20-npz/bmad-npz\"\n# DATASET_NAME = \"camelyon16_256\"\nDATASET_NAME = \"BraTS2021_slice\"\n# DATASET_NAME = \"hist_DIY\"\nDATASET_PATH = f\"{DATASET_DIR}/{DATASET_NAME}/{DATASET_NAME}_train.npz\"\nSYNTHETIC_DIR = f\"synthetics/\"\nos.makedirs(SYNTHETIC_DIR, exist_ok=True)\nMODEL_DIR = f\"{DATASET_NAME}_models/\"\nos.makedirs(MODEL_DIR, exist_ok=True)\nPATIENTCE = 5\nBATCH_SIZE = 100\nLATENT_DIM = 64\n\ntrain_dataset, train_loader, test_dataset, test_loader = load_train_test(\n    DATASET_PATH, device=DEVICE, batch_size=BATCH_SIZE, resize=256\n)\n\nenv = ContrastiveEnv(None, None, device=DEVICE)\nactor = Actor(input_dim=LATENT_DIM, z_dim=LATENT_DIM)\ncritic = Critic(input_dim=LATENT_DIM)\nppo = PPO(actor, critic, env, device=DEVICE)\n\ngenerator = CNNAutoEncoder(latent_dim=LATENT_DIM).to(DEVICE)\nembedder = ResNetEmbedder(output_dim=128).to(DEVICE)\ndetector = ViTransformerDetector(embedder=embedder).to(DEVICE)\nearly_stopper = EarlyStopping(patience=PATIENTCE)\n\noptimizer_g = torch.optim.Adam(generator.parameters(), lr=1e-4)\noptimizer_d = torch.optim.Adam(detector.parameters(), lr=1e-4)\ncriterion = nn.BCELoss()\n\nsynthetic_files = []\nnum_epochs_generator = 100\nnum_epochs_detector = 100\nnum_episodes = 100\nnum_gen_data = 50\nnum_steps_ppo = 100\ncombined_dataset = train_dataset\n\nfor ep in range(num_episodes):\n    print(f\"\\n================ EPISODE {ep+1}/{num_episodes} ================\")\n    total_class0, total_class1 = count_classes(combined_dataset)\n\n    # 2️⃣ Print\n    print(f\"[Episode {ep+1}] Class count → 0: {total_class0}, 1: {total_class1}\")\n\n    # 3️⃣ Auto-stop if balanced\n    if total_class1 >= total_class0:\n        print(\"Dataset is now balanced. Stopping synthetic generation.\")\n        break\n\n    # TRAIN GENERATOR\n    early_stopper.reset()\n    for epoch in range(num_epochs_generator):\n        loss_g = train_cnn_ae(generator, train_loader, optimizer_g, DEVICE)\n        print(f\"[GENERATOR] Epoch {epoch+1}/{num_epochs_generator}, Loss={loss_g:.4f}\")\n        early_stopper.step(loss_g)\n        if early_stopper.early_stop:\n            print(f\"[GENERATOR] Early stopping at epoch {epoch+1}\")\n            break\n\n    torch.save(generator.state_dict(), f\"{MODEL_DIR}/{DATASET_NAME}_generator.pth\")\n\n    # 4️⃣ Train detector\n    early_stopper.reset()\n    for epoch in range(num_epochs_detector):\n        loss_d = train_detector(detector, train_loader, optimizer_d, criterion, DEVICE)\n        print(f\"[DETECTOR] Epoch {epoch+1}/{num_epochs_detector}, Loss={loss_d:.4f}\")\n        early_stopper.step(loss_d)\n        if early_stopper.early_stop:\n            print(f\"[DETECTOR] Early stopping at epoch {epoch+1}\")\n            break\n\n    torch.save(detector.state_dict(), f\"{MODEL_DIR}/{DATASET_NAME}_detector.pth\")\n\n    # 5️⃣ Generate synthetic samples (class 1)\n    synthetic_samples = []\n    idx_class1 = [i for i, idx_lbl in enumerate(train_dataset.indices) if train_dataset.labels[idx_lbl] == 1]\n\n    for _ in range(num_gen_data):\n        if not idx_class1:\n            break\n        rand_idx = np.random.choice(idx_class1)\n        x_orig, _ = train_dataset[rand_idx]\n        x_adv = None\n        if ep != 0:\n            x_adv = Gen_with_PPO(ppo, generator, x_orig, DEVICE)\n        if x_adv is None:\n            x_adv = One_Step_To_Feasible_Action(generator, detector, x_orig, DEVICE)\n        synthetic_samples.append(x_adv.to(DEVICE))\n\n    syn_tensor = torch.cat(synthetic_samples)            # (N,3,H,W)\n    syn_tensor_hwc = syn_tensor.permute(0,2,3,1).cpu().numpy()   # (N,H,W,3)  <-- HWC\n    syn_labels = np.ones(len(synthetic_samples), dtype=np.float32)\n\n    syn_path = f\"{SYNTHETIC_DIR}/syn_ep{ep}.npz\"\n    np.savez_compressed(syn_path, images=syn_tensor_hwc, labels=syn_labels)\n    synthetic_files.append(syn_path)\n\n    print(f\"[EP {ep}] Generated synthetic saved:\", syn_path)\n\n    synthetic_subsets = []\n    for f in synthetic_files:\n        synthetic_subsets.append(SyntheticDataset(f, DEVICE))\n\n    synthetic_full = ConcatDataset(synthetic_subsets)\n    combined_dataset = ConcatDataset([train_dataset, synthetic_full])\n\n    train_loader = DataLoader(combined_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\n    Z_ppo, Y_ppo = load_z_space(generator, train_loader, DEVICE)\n    env.set_data(Z_ppo, Y_ppo)\n    # Training PPO\n    early_stopper.reset()\n    for step in range(num_steps_ppo):\n        batch_indices = list(range(len(Z_ppo)))  # hoặc chọn minibatch\n        a_loss, c_loss, avg_reward = ppo.learn(batch_indices)\n        print(f\"Step {step:03d} | Actor Loss = {a_loss:.4f}, Critic Loss = {c_loss:.4f}, AvgReward={avg_reward:.4f}\")\n        early_stopper.step(avg_reward)\n        if early_stopper.early_stop:\n            print(f\"[PPO] Early stopping at step {step+1}\")\n            break\n\n    print(f\"[EP {ep}] Train dataset expanded: now {len(combined_dataset)} samples\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T19:49:21.194281Z","iopub.execute_input":"2025-12-08T19:49:21.194531Z","execution_failed":"2025-12-08T19:52:14.848Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 219MB/s]\n","output_type":"stream"},{"name":"stdout","text":"\n================ EPISODE 1/2 ================\n[Episode 1] Class count → 0: 2617, 1: 998\n[GENERATOR] Epoch 1/2, Loss=0.1425\n[GENERATOR] Epoch 2/2, Loss=0.0715\n[DETECTOR] Epoch 1/2, Loss=0.3352\n[DETECTOR] Epoch 2/2, Loss=0.1378\n[EP 0] Generated synthetic saved: synthetics//syn_ep0.npz\nStep 000 | Actor Loss = 15.4556, Critic Loss = 614.5878, AvgReward=-10.9865\nStep 001 | Actor Loss = 11.9744, Critic Loss = 564.5591, AvgReward=-10.7074\n[EP 0] Train dataset expanded: now 3617 samples\n\n================ EPISODE 2/2 ================\n[Episode 2] Class count → 0: 2617, 1: 1000\n[GENERATOR] Epoch 1/2, Loss=0.0475\n[GENERATOR] Epoch 2/2, Loss=0.0335\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"final_detector = ViTransformerDetector(embedder=embedder).to(DEVICE)\noptimizer_final = torch.optim.Adam(final_detector.parameters(), lr=1e-3)\ncriterion = nn.BCELoss()\nnum_epochs_final = 100\nearly_stopper.reset()\n\nfor epoch in range(num_epochs_final):\n    loss = train_detector(final_detector, train_loader, optimizer_final, criterion, DEVICE)\n    print(f\"[FINAL DETECTOR] Epoch {epoch+1}/{num_epochs_final}, Loss={loss:.4f}\")\n\n    # Check early stopping\n    early_stopper.step(loss)\n\n    if early_stopper.early_stop:\n        print(f\"[FINAL DETECTOR] Early stopping at epoch {epoch+1}\")\n        break","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-08T19:52:14.848Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"evaluate_model(final_detector, test_loader, DEVICE, threshold=0.3)\n\n# Save final detector\ntorch.save(final_detector.state_dict(), f\"{MODEL_DIR}/{DATASET_NAME}_detector_final.pth\")\nprint(f\"Final detector saved to {MODEL_DIR}/{DATASET_NAME}_detector_final.pth\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-08T19:52:14.848Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\nsynthetics_folder = \"/kaggle/working/synthetics\"\nsynthetics_zip = f\"{DATASET_NAME}_synthetics\"\n\nshutil.make_archive(synthetics_zip, 'zip', synthetics_folder)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-08T19:52:14.848Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\nmodels_folder = f\"/kaggle/working/{DATASET_NAME}_models\"\nmodels_zip = f\"{DATASET_NAME}_models\"\n\nshutil.make_archive(models_zip, 'zip', models_folder)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-08T19:52:14.848Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"strict_dataset, strict_loader, _, _ = load_train_test(\n    f\"{DATASET_DIR}/{DATASET_NAME}/{DATASET_NAME}_train.npz\", device=DEVICE, batch_size=100, test_size=0\n)\n\n# Test set evaluation\nevaluate_model(final_detector, strict_loader, DEVICE, threshold=0.3)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-08T19:52:14.848Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"strict_dataset, strict_loader, _, _ = load_train_test(\n    f\"{DATASET_DIR}/{DATASET_NAME}/{DATASET_NAME}_valid.npz\", device=DEVICE, batch_size=100, test_size=0\n)\n\n# Test set evaluation\nevaluate_model(final_detector, strict_loader, DEVICE, threshold=0.3)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-08T19:52:14.849Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom torchvision import transforms\nfrom torch.utils.data import TensorDataset\n\ndef preview_images(loader, model=None, device='cpu', num_images=5):\n    for images, labels in loader:\n        images = images[:num_images]\n        labels = labels[:num_images]\n\n        if model is None:\n            # Chỉ hiển thị ảnh\n            imgs_np = images.cpu().numpy()\n            labels_np = labels.cpu().numpy()\n            imgs_np = np.clip(imgs_np, 0, 1)\n            n = imgs_np.shape[0]\n            fig, axes = plt.subplots(1, n, figsize=(3*n, 3))\n            if n == 1:\n                axes = [axes]\n            for j in range(n):\n                label = int(labels_np[j])\n                axes[j].imshow(imgs_np[j].transpose(1, 2, 0))\n                axes[j].set_title(f'[{label}] Image {j}')\n                axes[j].axis('off')\n            plt.tight_layout()\n            plt.show()\n        else:\n            # Hiển thị Original vs Reconstructed\n            model.eval()\n            with torch.no_grad():\n                images = images.to(device)\n                labels = labels.to(device)\n                reconstructed, _, _ = model(images, labels.unsqueeze(1))\n\n            orig = images.cpu().numpy()\n            recon = reconstructed.cpu().numpy()\n            labels_np = labels.cpu().numpy()\n            orig = np.clip(orig, 0, 1)\n            recon = np.clip(recon, 0, 1)\n\n            n = orig.shape[0]\n            fig, axes = plt.subplots(2, n, figsize=(4*n, 8))\n            if n == 1:\n                axes = axes.reshape(2, 1)\n\n            for j in range(n):\n                label = int(labels_np[j])\n                axes[0, j].imshow(orig[j].transpose(1, 2, 0))\n                axes[0, j].set_title(f'[{label}] Org-{j}')\n                axes[0, j].axis('off')\n\n                axes[1, j].imshow(recon[j].transpose(1, 2, 0))\n                axes[1, j].set_title(f'[{label}] Recon-{j}')\n                axes[1, j].axis('off')\n            plt.tight_layout()\n            plt.show()\n\n        break  # chỉ lấy 1 batch\n\ndef load_data(data, batch_num=64, is_shuffle=True):\n    images = data['images']  # shape: (N, H, W, C)\n    labels = data['labels']\n\n    # Define transform\n    transform = transforms.Compose([\n        transforms.ToPILImage(),            # Chuyển numpy array -> PIL Image\n        transforms.Resize((256, 256)),      # Resize về 256x256\n        transforms.ToTensor()               # Chuyển PIL Image -> Tensor [0,1]\n    ])\n\n    # Áp dụng transform cho từng ảnh\n    X = torch.stack([transform(img) for img in images])\n\n    # Chuyển labels sang tensor\n    y = torch.tensor(labels, dtype=torch.float32)\n\n    # Tạo dataset & dataloader\n    dataset = TensorDataset(X, y)\n    loader = DataLoader(dataset, batch_size=batch_num, shuffle=is_shuffle)\n\n    return loader","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-08T19:52:14.849Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"syn_data = np.load(\"/kaggle/working/synthetics/syn_ep9.npz\", allow_pickle=True)\nsyn_loader = load_data(syn_data)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-08T19:52:14.849Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preview_images(syn_loader)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-08T19:52:14.849Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"t_data = np.load(f\"{DATASET_DIR}/{DATASET_NAME}/{DATASET_NAME}_valid.npz\", allow_pickle=True)\nt_loader = load_data(t_data)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-08T19:52:14.849Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preview_images(t_loader)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-08T19:52:14.849Z"}},"outputs":[],"execution_count":null}]}