{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14276737,"sourceType":"datasetVersion","datasetId":9111559}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport random\nimport math\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, roc_auc_score\nfrom torchvision import models\nimport torch.optim as optim","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:17:48.105510Z","iopub.execute_input":"2026-01-14T08:17:48.105693Z","iopub.status.idle":"2026-01-14T08:17:56.024469Z","shell.execute_reply.started":"2026-01-14T08:17:48.105673Z","shell.execute_reply":"2026-01-14T08:17:56.023882Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"def set_seed(seed=42):\n    # Python\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n\n    # NumPy\n    np.random.seed(seed)\n\n    # PyTorch\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\nset_seed(11)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:17:56.026197Z","iopub.execute_input":"2026-01-14T08:17:56.026589Z","iopub.status.idle":"2026-01-14T08:17:56.036709Z","shell.execute_reply.started":"2026-01-14T08:17:56.026564Z","shell.execute_reply":"2026-01-14T08:17:56.036082Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# =========================================================\n# ENV\n# =========================================================\nclass LatentSpaceEnv:\n    def __init__(self, device=\"cpu\", max_steps=10, target_sim=0.7):\n        self.device = device\n        self.max_steps = max_steps\n        self.target_sim = target_sim\n\n        self.X = None\n        self.Y = None\n\n    # ================= DATA =================\n    def set_data(self, X, Y):\n        self.X = X.to(self.device)\n        self.Y = Y.to(self.device)\n\n        self.pos_idx = (self.Y == 1).nonzero(as_tuple=True)[0]\n        self.neg_idx = (self.Y == 0).nonzero(as_tuple=True)[0]\n\n        assert len(self.pos_idx) > 0, \"No label=1 samples\"\n\n    # ================= RESET =================\n    def reset(self):\n        self.current_idx = self.pos_idx[\n            torch.randint(len(self.pos_idx), (1,))\n        ].item()\n\n        self.x_orig = self.X[self.current_idx]\n        self.x_current = self.x_orig.clone()\n\n        self.step_count = 0\n        return self.get_state()\n\n    # ================= STATE =================\n    def get_state(self):\n        return self.x_current.clone()\n\n    # ================= REWARD =================\n    def compute_reward(self, x_prime):\n        pos_samples = self.X[self.pos_idx]\n        neg_samples = self.X[self.neg_idx]\n\n        mean_pos = torch.mean(torch.norm(x_prime - pos_samples, dim=1))\n        mean_neg = torch.mean(torch.norm(x_prime - neg_samples, dim=1))\n        class_margin = torch.sigmoid(mean_neg - mean_pos)\n\n        cos_pos = F.cosine_similarity(\n            x_prime.unsqueeze(0), pos_samples, dim=1\n        )\n        max_sim = torch.max(cos_pos)\n        diversity_penalty = torch.relu(max_sim - 0.95)\n\n        sim_orig = F.cosine_similarity(x_prime, self.x_orig, dim=0)\n        target_reward = torch.exp(\n            -((sim_orig - self.target_sim) ** 2) / 0.02\n        )\n\n        reward = (\n            0.5 * class_margin\n            + 0.4 * target_reward\n            - 0.6 * diversity_penalty\n        )\n\n        return reward  # scalar\n\n    # ================= STEP =================\n    def step(self, action):\n        self.step_count += 1\n\n        x_prime = self.x_current + action\n        reward = self.compute_reward(x_prime)\n\n        self.x_current = x_prime.detach()\n        done = self.step_count >= self.max_steps\n\n        return self.get_state(), reward, done\n\n\n# =========================================================\n# ACTOR / CRITIC\n# =========================================================\nclass Actor(nn.Module):\n    def __init__(self, latent_dim, hidden=256):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(latent_dim, hidden),\n            nn.ReLU(),\n            nn.Linear(hidden, hidden),\n            nn.ReLU()\n        )\n        self.mu = nn.Linear(hidden, latent_dim)\n        self.logstd = nn.Parameter(torch.zeros(latent_dim))\n\n    def forward(self, x):\n        h = self.net(x)\n        std = self.logstd.exp()\n        return self.mu(h), std\n\n\nclass Critic(nn.Module):\n    def __init__(self, latent_dim, hidden=256):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(latent_dim, hidden),\n            nn.ReLU(),\n            nn.Linear(hidden, hidden),\n            nn.ReLU(),\n            nn.Linear(hidden, 1)\n        )\n\n    def forward(self, x):\n        return self.net(x).squeeze(-1)\n\n\n# =========================================================\n# ROLLOUT BUFFER\n# =========================================================\nclass RolloutBuffer:\n    def __init__(self):\n        self.clear()\n\n    def clear(self):\n        self.states = []\n        self.actions = []\n        self.logps = []\n        self.rewards = []\n        self.values = []\n        self.dones = []\n\n    def add(self, s, a, logp, r, v, d):\n        self.states.append(s)\n        self.actions.append(a)\n        self.logps.append(logp)\n        self.rewards.append(r)\n        self.values.append(v)\n        self.dones.append(d)\n\n\n# =========================================================\n# PPO\n# =========================================================\nclass PPO:\n    def __init__(\n        self,\n        actor,\n        critic,\n        env,\n        lr=3e-4,\n        gamma=0.99,\n        lam=0.95,\n        clip_eps=0.2,\n        train_epochs=5,\n        device=\"cpu\"\n    ):\n        self.actor = actor.to(device)\n        self.critic = critic.to(device)\n        self.env = env\n\n        self.gamma = gamma\n        self.lam = lam\n        self.clip_eps = clip_eps\n        self.train_epochs = train_epochs\n        self.device = device\n\n        self.opt_actor = optim.Adam(self.actor.parameters(), lr=lr)\n        self.opt_critic = optim.Adam(self.critic.parameters(), lr=lr)\n\n        self.buffer = RolloutBuffer()\n\n    # ================= ROLLOUT =================\n    def collect_trajectories(self, num_episodes=4):\n        self.buffer.clear()\n        episode_rewards = []\n\n        for _ in range(num_episodes):\n            state = self.env.reset()\n            ep_reward = 0.0\n\n            for _ in range(self.env.max_steps):\n                state = state.to(self.device)\n\n                mu, std = self.actor(state)\n                dist = torch.distributions.Normal(mu, std)\n\n                action = dist.sample()\n                logp = dist.log_prob(action).sum(dim=-1)\n\n                value = self.critic(state)\n\n                next_state, reward, done = self.env.step(action)\n\n                self.buffer.add(\n                    state.detach(),\n                    action.detach(),\n                    logp.detach(),\n                    reward.detach(),\n                    value.detach(),\n                    done\n                )\n\n                state = next_state\n                ep_reward += reward.item()\n\n                if done:\n                    break\n\n            episode_rewards.append(ep_reward)\n\n        return {\"avg_reward\": float(np.mean(episode_rewards))}\n\n    # ================= GAE =================\n    def compute_gae(self):\n        rewards = self.buffer.rewards\n        values = self.buffer.values\n        dones = self.buffer.dones\n\n        advantages = []\n        returns = []\n\n        gae = 0\n        next_value = 0\n\n        for t in reversed(range(len(rewards))):\n            mask = 1.0 - float(dones[t])\n            delta = rewards[t] + self.gamma * next_value * mask - values[t]\n            gae = delta + self.gamma * self.lam * mask * gae\n\n            advantages.insert(0, gae)\n            returns.insert(0, gae + values[t])\n\n            next_value = values[t]\n\n        return torch.stack(advantages), torch.stack(returns)\n\n    # ================= UPDATE =================\n    def learn(self):\n        states = torch.stack(self.buffer.states)\n        actions = torch.stack(self.buffer.actions)\n        old_logps = torch.stack(self.buffer.logps)\n\n        advantages, returns = self.compute_gae()\n        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n\n        for _ in range(self.train_epochs):\n            mu, std = self.actor(states)\n            dist = torch.distributions.Normal(mu, std)\n            logps = dist.log_prob(actions).sum(dim=-1)\n\n            ratio = torch.exp(logps - old_logps)\n\n            surr1 = ratio * advantages\n            surr2 = torch.clamp(\n                ratio, 1 - self.clip_eps, 1 + self.clip_eps\n            ) * advantages\n\n            actor_loss = -torch.min(surr1, surr2).mean()\n\n            values = self.critic(states)\n            critic_loss = F.mse_loss(values, returns)\n\n            self.opt_actor.zero_grad()\n            actor_loss.backward()\n            self.opt_actor.step()\n\n            self.opt_critic.zero_grad()\n            critic_loss.backward()\n            self.opt_critic.step()\n\n        avg_reward = torch.stack(self.buffer.rewards).mean().item()\n        return actor_loss.item(), critic_loss.item(), avg_reward\n\n    # ================= INFER =================\n    @torch.no_grad()\n    def infer(self, z0, steps=None, step_scale=0.1):\n        self.actor.eval()\n\n        if steps is None:\n            steps = self.env.max_steps\n\n        z = z0.clone().to(self.device)\n\n        for _ in range(steps):\n            mu, _ = self.actor(z)\n            z = z + step_scale * torch.tanh(mu)\n\n        return z\n\n\n\n# =========================================================\n# OFFLINE REFINEMENT\n# =========================================================\n@torch.no_grad()\ndef refine_latent_offline(actor, x0, steps=8, step_scale=0.1):\n    x = x0.clone()\n    for _ in range(steps):\n        mu, _ = actor(x)\n        x = x + step_scale * torch.tanh(mu)\n    return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:17:56.037704Z","iopub.execute_input":"2026-01-14T08:17:56.038080Z","iopub.status.idle":"2026-01-14T08:17:56.095883Z","shell.execute_reply.started":"2026-01-14T08:17:56.038051Z","shell.execute_reply":"2026-01-14T08:17:56.095181Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class NPZDataset(Dataset):\n    def __init__(self, path, indices=None, resize=256, device=\"cpu\"):\n        self.data = np.load(path, allow_pickle=True, mmap_mode=\"r\")\n        self.images = self.data[\"images\"]\n        self.labels = self.data[\"labels\"]\n\n        if indices is None:\n            self.indices = np.arange(len(self.labels))\n        else:\n            self.indices = indices\n\n        self.resize = resize\n        self.device = device\n\n    def __len__(self):\n        return len(self.indices)\n\n    def __getitem__(self, idx):\n        i = self.indices[idx]\n        img = torch.tensor(self.images[i]).float()\n        lbl = torch.tensor(self.labels[i]).float()\n\n        img = img.permute(2,0,1)\n        if img.max() > 1:\n            img = img / 255.\n\n        img = F.interpolate(\n            img.unsqueeze(0),\n            size=(self.resize, self.resize),\n            mode=\"bilinear\",\n            align_corners=False\n        ).squeeze(0)\n\n        return img.to(self.device), lbl.to(self.device)\n\nclass SyntheticDataset(Dataset):\n    def __init__(self, path, device):\n        self.data = np.load(path, mmap_mode=\"r\")\n        self.images = self.data[\"images\"]\n        self.labels = self.data[\"labels\"]\n        self.device = device\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        img = torch.from_numpy(self.images[idx]).float().permute(2,0,1)\n        lbl = torch.tensor(self.labels[idx], dtype=torch.float32)\n        return img.to(self.device), lbl.to(self.device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:17:56.096823Z","iopub.execute_input":"2026-01-14T08:17:56.097273Z","iopub.status.idle":"2026-01-14T08:17:56.113591Z","shell.execute_reply.started":"2026-01-14T08:17:56.097239Z","shell.execute_reply":"2026-01-14T08:17:56.113061Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def load_train_test(\n    path,\n    device=\"cpu\",\n    batch_size=32,\n    resize=256,\n    test_size=0.6,\n    ratio_0=20/85,\n    seed=42\n):\n    data = np.load(path, mmap_mode=\"r\")\n    labels = data[\"labels\"]\n\n    # ---- lọc nhãn ----\n    idx_1 = np.where(labels == 1)[0]\n    idx_0 = np.where(labels == 0)[0]\n\n    n0 = int(len(idx_0) * ratio_0)\n    np.random.seed(seed)\n    idx_0 = np.random.choice(idx_0, n0, replace=False)\n\n    indices = np.concatenate([idx_1, idx_0])\n    np.random.shuffle(indices)\n\n    # ---- split ----\n    if test_size > 0:\n        train_idx, test_idx = train_test_split(\n            indices,\n            test_size=test_size,\n            stratify=labels[indices],\n            random_state=seed\n        )\n    else:\n        train_idx = indices\n        test_idx = []\n\n    train_dataset = NPZDataset(path, train_idx, resize, device)\n    test_dataset  = NPZDataset(path, test_idx,  resize, device) if len(test_idx) > 0 else None\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader  = DataLoader(test_dataset, batch_size=batch_size) if test_dataset else None\n\n    return train_dataset, train_loader, test_dataset, test_loader\n\ndef count_classes(dataset):\n    count0 = 0\n    count1 = 0\n\n    for i in range(len(dataset)):\n        lbl = dataset[i][1]    # dataset[i] → (image, label)\n        lbl = int(lbl.item())  # convert to 0 hoặc 1\n\n        if lbl == 0:\n            count0 += 1\n        else:\n            count1 += 1\n    return count0, count1\n\ndef evaluate_model(model, loader, device, threshold=0.5):\n    model.eval()\n    all_preds, all_labels = [], []\n\n    with torch.no_grad():\n        for x_batch, y_batch in loader:\n            x_batch = x_batch.to(device)\n            y_batch = y_batch.to(device)\n            y_pred = model(x_batch).squeeze()\n            all_preds.append(y_pred.cpu())\n            all_labels.append(y_batch.cpu())\n\n    preds = torch.cat(all_preds).numpy()\n    labels = torch.cat(all_labels).numpy()\n\n\n    binary_pred = (preds > threshold).astype(int)\n\n    print(\"Classification Report:\")\n    print(classification_report(labels, binary_pred, labels=[0,1], target_names=[\"Class 0\",\"Class 1\"]))\n\n    if len(set(labels)) > 1:\n        auc = roc_auc_score(labels, preds)\n        print(f\"AUC-ROC: {auc:.4f}\")\n    else:\n        print(\"AUC-ROC: Undefined (only one class present)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:17:56.114510Z","iopub.execute_input":"2026-01-14T08:17:56.114757Z","iopub.status.idle":"2026-01-14T08:17:56.132018Z","shell.execute_reply.started":"2026-01-14T08:17:56.114728Z","shell.execute_reply":"2026-01-14T08:17:56.131425Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class ResNetEmbedder(nn.Module):\n    def __init__(self, output_dim=128, pretrained=True):\n        super().__init__()\n        self.backbone = models.resnet50(pretrained=pretrained)\n        self.backbone.fc = nn.Identity()\n        self.dim_adjuster = nn.Linear(2048, output_dim)\n        self.output_dim = output_dim\n\n    def forward(self, images):\n        features = self.backbone(images)\n        features = features.squeeze()\n        if features.dim() == 1:\n            features = features.unsqueeze(0)\n        features = self.dim_adjuster(features)\n        return features\n\nclass CNNAutoEncoder(nn.Module):\n    def __init__(self, latent_dim=128):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            nn.Conv2d(3,64,4,2,1), nn.BatchNorm2d(64), nn.ReLU(),\n            nn.Conv2d(64,128,4,2,1), nn.BatchNorm2d(128), nn.ReLU(),\n            nn.Conv2d(128,256,4,2,1), nn.BatchNorm2d(256), nn.ReLU(),\n            nn.Conv2d(256,512,4,2,1), nn.BatchNorm2d(512), nn.ReLU()\n        )\n        self.encoder2latent = nn.Linear(512*16*16+1, latent_dim)\n        self.latent2decoder = nn.Linear(latent_dim+1, 512*16*16)\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(512,256,4,2,1), nn.BatchNorm2d(256), nn.ReLU(),\n            nn.ConvTranspose2d(256,128,4,2,1), nn.BatchNorm2d(128), nn.ReLU(),\n            nn.ConvTranspose2d(128,64,4,2,1), nn.BatchNorm2d(64), nn.ReLU(),\n            nn.ConvTranspose2d(64,3,4,2,1), nn.Sigmoid()\n        )\n\n    def encode(self, x, y):\n        h = self.encoder(x)\n        flat = h.view(x.size(0), -1)\n        xy = torch.cat([flat, y], dim=1)\n        z = self.encoder2latent(xy)\n        return z\n\n    def decode(self, z, y):\n        zy = torch.cat([z, y], dim=1)\n        flat = self.latent2decoder(zy)\n        h = flat.view(z.size(0),512,16,16)\n        x_recon = self.decoder(h)\n        return x_recon\n\n    def forward(self, x, y):\n        z = self.encode(x, y)\n        return self.decode(z, y)\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0,max_len).unsqueeze(1).float()\n        div_term = torch.exp(torch.arange(0,d_model,2).float()*(-np.log(10000.0)/d_model))\n        pe[:,0::2] = torch.sin(position*div_term)\n        pe[:,1::2] = torch.cos(position*div_term)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self,x):\n        L = x.size(1)\n        return x + self.pe[:,:L,:]\n\nclass ViTransformerDetector(nn.Module):\n    def __init__(self, embedder, d_model=128, nhead=8, num_layers=2, dim_feedforward=256, dropout=0.1):\n        super().__init__()\n        self.embedder = embedder\n        self.embedding_dim = embedder.output_dim\n        self.projection = nn.Linear(self.embedding_dim,d_model)\n        self.positional_encoding = PositionalEncoding(d_model)\n        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,\n                                                   dim_feedforward=dim_feedforward,\n                                                   dropout=dropout, batch_first=True)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.fc = nn.Sequential(\n            nn.Linear(d_model,128), nn.ReLU(), nn.Dropout(dropout),\n            nn.Linear(128,1), nn.Sigmoid()\n        )\n\n    def forward(self,x):\n        emb = self.embedder(x).view(x.size(0),1,-1)\n        emb = self.projection(emb)\n        emb = self.positional_encoding(emb)\n        h = self.transformer_encoder(emb)\n        pooled = h.mean(dim=1)\n        return self.fc(pooled).squeeze(1)\n\nclass EarlyStopping:\n    def __init__(self, patience=5, min_delta=1e-4):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.best_loss = float(\"inf\")\n        self.early_stop = False\n\n    def step(self, current_loss):\n        if current_loss < self.best_loss - self.min_delta:\n            self.best_loss = current_loss\n            self.counter = 0\n        else:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True\n\n    def reset(self):\n        self.counter = 0\n        self.best_loss = float(\"inf\")\n        self.early_stop = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:17:56.132999Z","iopub.execute_input":"2026-01-14T08:17:56.133318Z","iopub.status.idle":"2026-01-14T08:17:56.151158Z","shell.execute_reply.started":"2026-01-14T08:17:56.133294Z","shell.execute_reply":"2026-01-14T08:17:56.150471Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def train_cnn_ae(model, loader, opt, device):\n    model.train()\n    total = 0\n    for x,y in loader:\n        x = x.to(device)\n        y = y.unsqueeze(1).to(device)\n        recon = model.decode(model.encode(x,y), y)\n        loss = torch.mean(torch.abs(x - recon))\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n        total += loss.item()\n    return total/len(loader)\n\ndef train_detector(model, loader, opt, criterion, device):\n    model.train()\n    total = 0\n    for x,y in loader:\n        x=x.to(device); y=y.to(device)\n        out = model(x)\n        loss = criterion(out, y)\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n        total += loss.item()\n    return total/len(loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:17:56.153429Z","iopub.execute_input":"2026-01-14T08:17:56.153641Z","iopub.status.idle":"2026-01-14T08:17:56.166825Z","shell.execute_reply.started":"2026-01-14T08:17:56.153621Z","shell.execute_reply":"2026-01-14T08:17:56.166231Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def One_Step_To_Feasible_Action(generator, detector, x_orig, device,\n                                previously_generated=None, alpha=1.0,\n                                lambda_div=0.1, lr=0.001, steps=50):\n    generator.eval()\n    detector.eval()\n    if previously_generated is None:\n        previously_generated = []\n\n    x_orig = x_orig.to(device).unsqueeze(0)\n    y_class1 = torch.full((1,1),0.8,device=device)\n\n    with torch.no_grad():\n        z = generator.encode(x_orig, y_class1)\n    z = z.clone().detach().requires_grad_(True)\n\n    optimizer_z = torch.optim.Adam([z], lr=lr)\n\n    for step in range(steps):\n        optimizer_z.zero_grad()\n        x_syn = generator.decode(z, y_class1)\n        prob_class1 = detector(x_syn)\n\n        if len(previously_generated) > 0:\n            old = torch.stack(previously_generated).to(device)\n            diff = x_syn.unsqueeze(1)-old.unsqueeze(0)\n            dist_min = diff.norm(p=2,dim=(2,3,4)).min(dim=1).values\n            diversity_term = torch.exp(-alpha*dist_min).mean()\n        else:\n            diversity_term = torch.tensor(0.0, device=device)\n\n        reward = prob_class1.mean() + lambda_div*diversity_term\n        reward.backward()\n        optimizer_z.step()\n\n    with torch.no_grad():\n        x_adv = generator.decode(z, y_class1).detach().cpu()\n\n    return x_adv\n\ndef Gen_with_PPO(\n    actor,\n    generator,\n    x_orig,\n    device,\n    episodes,\n    steps=8\n):\n    actor.eval()\n    generator.eval()\n\n    # ===== y_class1 GIỮ NGUYÊN =====\n    conf = 0.8 + 0.1 / (1 + math.exp(-0.1 * (episodes - 10)))\n    y_class1 = torch.full(\n        (1, 1),\n        random.uniform(max(conf - 0.02, 0.8), conf),\n        device=device\n    )\n\n    with torch.no_grad():\n        z0 = generator.encode(\n            x_orig.unsqueeze(0).to(device),\n            y_class1\n        )\n\n    # ✅ ĐÚNG TÊN HÀM\n    z_adv = refine_latent_offline(\n        actor,\n        z0.squeeze(0),\n        steps=steps\n    )\n\n    if not check_valid_variant(z_adv, z0.squeeze(0)):\n        return None\n\n    with torch.no_grad():\n        x_synthetic = generator.decode(\n            z_adv.unsqueeze(0),\n            y_class1\n        )\n\n    return x_synthetic\n\n\ndef check_valid_variant(x, x_prime, threshold=0.3):\n    # Flatten nếu có batch dimension\n    if x.dim() > 1:\n        x_flat = x.view(x.size(0), -1)\n        x_prime_flat = x_prime.view(x_prime.size(0), -1)\n        cos_sim = torch.nn.functional.cosine_similarity(x_flat, x_prime_flat, dim=1)\n    else:\n        cos_sim = torch.nn.functional.cosine_similarity(x, x_prime, dim=0)\n    \n    cos_dist = 1 - cos_sim\n    \n    # Trả về boolean value duy nhất\n    # Nếu có nhiều samples, kiểm tra xem TẤT CẢ có valid không\n    is_valid = (cos_dist < threshold).all()\n    \n    return is_valid.item()  # Convert tensor to Python bool\n\ndef load_z_space(generator, loader, device):\n    generator.eval()\n\n    all_z = []\n    all_y = []\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device)\n            y = y.unsqueeze(1).to(device)\n\n            z = generator.encode(x, y)   # (batch, z_dim)\n\n            all_z.append(z.cpu())\n            all_y.append(y.cpu())\n\n    # Ghép toàn bộ batch lại\n    all_z = torch.cat(all_z, dim=0)\n    all_y = torch.cat(all_y, dim=0)\n\n    return all_z, all_y\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:17:56.167625Z","iopub.execute_input":"2026-01-14T08:17:56.167969Z","iopub.status.idle":"2026-01-14T08:17:56.181994Z","shell.execute_reply.started":"2026-01-14T08:17:56.167944Z","shell.execute_reply":"2026-01-14T08:17:56.181387Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nDATASET_DIR = \"/kaggle/input/anomaly-dataset-npz/npz-dataset/BMAD-AD\"\n# DATASET_NAME = \"RESC\"\nDATASET_NAME = \"OCT2017\"\n# DATASET_NAME = \"hist_DIY\"\nDATASET_PATH = f\"{DATASET_DIR}/{DATASET_NAME}/BMAD-AD_{DATASET_NAME}_train.npz\"\nSYNTHETIC_DIR = f\"synthetics/\"\nos.makedirs(SYNTHETIC_DIR, exist_ok=True)\nMODEL_DIR = f\"{DATASET_NAME}_models/\"\nos.makedirs(MODEL_DIR, exist_ok=True)\nPATIENTCE = 5\nBATCH_SIZE = 100\nLATENT_DIM = 64\nREFINE_STEP = 10\n\ntrain_dataset, train_loader, test_dataset, test_loader = load_train_test(\n    DATASET_PATH, device=DEVICE, batch_size=BATCH_SIZE, resize=256\n)\n\n# # ===== ENV =====\n# env = LatentSpaceEnv(\n#     device=DEVICE,\n#     max_steps=REFINE_STEP\n# )\n\n# # ===== ACTOR / CRITIC =====\n# actor = Actor(\n#     latent_dim=LATENT_DIM,\n#     hidden=256\n# )\n\n# critic = Critic(\n#     latent_dim=LATENT_DIM,\n#     hidden=256\n# )\n\n# # ===== PPO =====\n# ppo = PPO(\n#     actor=actor,\n#     critic=critic,\n#     env=env,\n#     lr=3e-4,\n#     gamma=0.99,\n#     lam=0.95,\n#     clip_eps=0.2,\n#     device=DEVICE\n# )\n\n# generator = CNNAutoEncoder(latent_dim=LATENT_DIM).to(DEVICE)\nembedder = ResNetEmbedder(output_dim=128).to(DEVICE)\n# detector = ViTransformerDetector(embedder=embedder).to(DEVICE)\nearly_stopper = EarlyStopping(patience=PATIENTCE)\n\n# optimizer_g = torch.optim.Adam(generator.parameters(), lr=1e-4)\n# optimizer_d = torch.optim.Adam(detector.parameters(), lr=1e-4)\n# criterion = nn.BCELoss()\n\n# synthetic_files = []\n# num_epochs_generator = 100\n# num_epochs_detector = 100\n# num_episodes = 100\n# num_steps_ppo = 100\n# num_gen_data = 100\n# combined_dataset = train_dataset\n\n# for ep in range(num_episodes):\n#     print(f\"\\n================ EPISODE {ep+1}/{num_episodes} ================\")\n#     total_class0, total_class1 = count_classes(combined_dataset)\n\n#     # 2️⃣ Print\n#     print(f\"[Episode {ep+1}] Class count → 0: {total_class0}, 1: {total_class1}\")\n\n#     # 3️⃣ Auto-stop if balanced\n#     if total_class1 >= total_class0:\n#         print(\"Dataset is now balanced. Stopping synthetic generation.\")\n#         break\n\n#     # TRAIN GENERATOR\n#     early_stopper.reset()\n#     for epoch in range(num_epochs_generator):\n#         loss_g = train_cnn_ae(generator, train_loader, optimizer_g, DEVICE)\n#         if (epoch + 1) % 10 == 0:\n#             print(f\"[GENERATOR] Epoch {epoch+1}/{num_epochs_generator}, Loss={loss_g:.4f}\")\n#         early_stopper.step(loss_g)\n#         if early_stopper.early_stop:\n#             print(f\"[GENERATOR] Early stopping at epoch {epoch+1}\")\n#             print(f\"[GENERATOR] Epoch {epoch+1}/{num_epochs_generator}, Loss={loss_g:.4f}\")\n#             break\n\n#     torch.save(generator.state_dict(), f\"{MODEL_DIR}/{DATASET_NAME}_generator.pth\")\n\n#     # 4️⃣ Train detector\n    \n#     # early_stopper.reset()\n#     # for epoch in range(num_epochs_detector):\n#     #     loss_d = train_detector(detector, train_loader, optimizer_d, criterion, DEVICE)\n#     #     if (epoch + 1) % 10 == 0:\n#     #         print(f\"[DETECTOR] Epoch {epoch+1}/{num_epochs_detector}, Loss={loss_d:.4f}\")\n#     #     early_stopper.step(loss_d)\n#     #     if early_stopper.early_stop:\n#     #         print(f\"[DETECTOR] Early stopping at epoch {epoch+1}\")\n#     #         print(f\"[DETECTOR] Epoch {epoch+1}/{num_epochs_detector}, Loss={loss_d:.4f}\")\n#     #         break\n\n#     # torch.save(detector.state_dict(), f\"{MODEL_DIR}/{DATASET_NAME}_detector.pth\")\n\n\n#     # 5️⃣ Generate synthetic samples (class 1)\n#     synthetic_samples = []\n#     idx_class1 = [i for i, idx_lbl in enumerate(train_dataset.indices) if train_dataset.labels[idx_lbl] == 1]\n\n#     ppo_gen_count = 0\n#     for _ in range(num_gen_data):\n#         if not idx_class1:\n#             break\n#         rand_idx = np.random.choice(idx_class1)\n#         x_orig, _ = train_dataset[rand_idx]\n#         x_adv = None\n#         if ep != 0:\n#             x_adv = Gen_with_PPO(actor, generator, x_orig, DEVICE, ep+1, REFINE_STEP)\n#         if x_adv is None:\n#             x_adv = One_Step_To_Feasible_Action(generator, detector, x_orig, DEVICE)\n#         else:\n#             ppo_gen_count += 1\n#         synthetic_samples.append(x_adv.to(DEVICE))\n\n#     ppo_gen_rate  = (ppo_gen_count / num_gen_data) * 100\n#     print(f\"{ppo_gen_rate:.2f}% of data generated by PPO strategy\")\n    \n#     syn_tensor = torch.cat(synthetic_samples)            # (N,3,H,W)\n#     syn_tensor_hwc = syn_tensor.permute(0,2,3,1).cpu().numpy()   # (N,H,W,3)  <-- HWC\n#     syn_labels = np.ones(len(synthetic_samples), dtype=np.float32)\n\n#     syn_path = f\"{SYNTHETIC_DIR}/syn_ep{ep+1}.npz\"\n#     np.savez_compressed(syn_path, images=syn_tensor_hwc, labels=syn_labels)\n#     synthetic_files.append(syn_path)\n\n#     print(f\"[EP {ep+1}] Generated synthetic saved:\", syn_path)\n\n#     synthetic_subsets = []\n#     for f in synthetic_files:\n#         synthetic_subsets.append(SyntheticDataset(f, DEVICE))\n\n#     synthetic_full = ConcatDataset(synthetic_subsets)\n#     combined_dataset = ConcatDataset([train_dataset, synthetic_full])\n\n#     train_loader = DataLoader(combined_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\n#     Z_ppo, Y_ppo = load_z_space(generator, train_loader, DEVICE)\n#     # env.set_models(generator, detector)\n#     env.set_data(Z_ppo, Y_ppo)\n#     # ===== TRAIN PPO =====\n#     early_stopper.reset()\n    \n#     for step in range(num_steps_ppo):\n    \n#         rollout_stats = ppo.collect_trajectories(\n#             num_episodes=4   # ✅ FIX CỨNG, KHÔNG DÙNG env.batch_size\n#         )\n    \n#         a_loss, c_loss, avg_reward = ppo.learn()\n    \n#         if (step + 1) % 1 == 0:\n#             print(\n#                 f\"Step {step:03d} | \"\n#                 f\"Actor Loss = {a_loss:.4f}, \"\n#                 f\"Critic Loss = {c_loss:.4f}, \"\n#                 f\"AvgReward = {avg_reward:.4f}\"\n#             )\n    \n#         early_stopper.step(-avg_reward)\n#         if early_stopper.early_stop:\n#             print(f\"[PPO] Early stopping at step {step+1}\")\n#             print(\n#                 f\"Step {step:03d} | \"\n#                 f\"Actor Loss = {a_loss:.4f}, \"\n#                 f\"Critic Loss = {c_loss:.4f}, \"\n#                 f\"AvgReward = {avg_reward:.4f}\"\n#             )\n#             break\n\n    \n#     print(f\"[EP {ep+1}] Train dataset expanded: now {len(combined_dataset)} samples\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:17:56.182838Z","iopub.execute_input":"2026-01-14T08:17:56.183150Z","iopub.status.idle":"2026-01-14T08:18:33.136480Z","shell.execute_reply.started":"2026-01-14T08:17:56.183130Z","shell.execute_reply":"2026-01-14T08:18:33.135593Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 97.8M/97.8M [00:00<00:00, 213MB/s]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"final_detector = ViTransformerDetector(embedder=embedder).to(DEVICE)\noptimizer_final = torch.optim.Adam(final_detector.parameters(), lr=1e-3)\ncriterion = nn.BCELoss()\nnum_epochs_final = 100\nearly_stopper.reset()\n\nfor epoch in range(num_epochs_final):\n    loss = train_detector(final_detector, train_loader, optimizer_final, criterion, DEVICE)\n    print(f\"[FINAL DETECTOR] Epoch {epoch+1}/{num_epochs_final}, Loss={loss:.4f}\")\n\n    # Check early stopping\n    early_stopper.step(loss)\n\n    if early_stopper.early_stop:\n        print(f\"[FINAL DETECTOR] Early stopping at epoch {epoch+1}\")\n        print(f\"[FINAL DETECTOR] Epoch {epoch+1}/{num_epochs_final}, Loss={loss:.4f}\")\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:18:33.137554Z","iopub.execute_input":"2026-01-14T08:18:33.137844Z","iopub.status.idle":"2026-01-14T08:22:17.100773Z","shell.execute_reply.started":"2026-01-14T08:18:33.137816Z","shell.execute_reply":"2026-01-14T08:22:17.100035Z"}},"outputs":[{"name":"stdout","text":"[FINAL DETECTOR] Epoch 1/100, Loss=0.2556\n[FINAL DETECTOR] Epoch 2/100, Loss=0.3218\n[FINAL DETECTOR] Epoch 3/100, Loss=0.1606\n[FINAL DETECTOR] Epoch 4/100, Loss=0.1156\n[FINAL DETECTOR] Epoch 5/100, Loss=0.0600\n[FINAL DETECTOR] Epoch 6/100, Loss=0.0647\n[FINAL DETECTOR] Epoch 7/100, Loss=0.0351\n[FINAL DETECTOR] Epoch 8/100, Loss=0.0187\n[FINAL DETECTOR] Epoch 9/100, Loss=0.0115\n[FINAL DETECTOR] Epoch 10/100, Loss=0.0299\n[FINAL DETECTOR] Epoch 11/100, Loss=0.0341\n[FINAL DETECTOR] Epoch 12/100, Loss=0.0287\n[FINAL DETECTOR] Epoch 13/100, Loss=0.1846\n[FINAL DETECTOR] Epoch 14/100, Loss=0.2376\n[FINAL DETECTOR] Early stopping at epoch 14\n[FINAL DETECTOR] Epoch 14/100, Loss=0.2376\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"evaluate_model(final_detector, test_loader, DEVICE, threshold=0.3)\n\n# Save final detector\ntorch.save(final_detector.state_dict(), f\"{MODEL_DIR}/{DATASET_NAME}_detector_final.pth\")\nprint(f\"Final detector saved to {MODEL_DIR}/{DATASET_NAME}_detector_final.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:22:17.101707Z","iopub.execute_input":"2026-01-14T08:22:17.102004Z","iopub.status.idle":"2026-01-14T08:22:27.321740Z","shell.execute_reply.started":"2026-01-14T08:22:17.101980Z","shell.execute_reply":"2026-01-14T08:22:27.321137Z"}},"outputs":[{"name":"stdout","text":"Classification Report:\n              precision    recall  f1-score   support\n\n     Class 0       0.97      0.03      0.07      3000\n     Class 1       0.11      0.99      0.20       360\n\n    accuracy                           0.14      3360\n   macro avg       0.54      0.51      0.13      3360\nweighted avg       0.88      0.14      0.08      3360\n\nAUC-ROC: 0.7200\nFinal detector saved to OCT2017_models//OCT2017_detector_final.pth\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# torch.save(actor.state_dict(), f\"{MODEL_DIR}/{DATASET_NAME}_actor.pth\")\n# print(f\"PPO Actor saved to {MODEL_DIR}/{DATASET_NAME}_actor.pth\")\n\n# torch.save(critic.state_dict(), f\"{MODEL_DIR}/{DATASET_NAME}_critic.pth\")\n# print(f\"PPO Critic saved to {MODEL_DIR}/{DATASET_NAME}_critic.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:22:27.322543Z","iopub.execute_input":"2026-01-14T08:22:27.322862Z","iopub.status.idle":"2026-01-14T08:22:27.326360Z","shell.execute_reply.started":"2026-01-14T08:22:27.322838Z","shell.execute_reply":"2026-01-14T08:22:27.325636Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# import shutil\n\n# synthetics_folder = \"/kaggle/working/synthetics\"\n# synthetics_zip = f\"{DATASET_NAME}_synthetics\"\n\n# shutil.make_archive(synthetics_zip, 'zip', synthetics_folder)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:22:27.327261Z","iopub.execute_input":"2026-01-14T08:22:27.327491Z","iopub.status.idle":"2026-01-14T08:22:27.337704Z","shell.execute_reply.started":"2026-01-14T08:22:27.327471Z","shell.execute_reply":"2026-01-14T08:22:27.337033Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import shutil\n\nmodels_folder = f\"/kaggle/working/{DATASET_NAME}_models\"\nmodels_zip = f\"{DATASET_NAME}_models1\"\n\nshutil.make_archive(models_zip, 'zip', models_folder)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:22:27.338579Z","iopub.execute_input":"2026-01-14T08:22:27.338825Z","iopub.status.idle":"2026-01-14T08:22:32.237564Z","shell.execute_reply.started":"2026-01-14T08:22:27.338805Z","shell.execute_reply":"2026-01-14T08:22:32.236959Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/OCT2017_models1.zip'"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"strict_dataset, strict_loader, _, _ = load_train_test(\n    f\"{DATASET_DIR}/{DATASET_NAME}/BMAD-AD_{DATASET_NAME}_train.npz\", device=DEVICE, batch_size=100, test_size=0\n)\n\n# Test set evaluation\nevaluate_model(final_detector, strict_loader, DEVICE, threshold=0.3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:22:32.238427Z","iopub.execute_input":"2026-01-14T08:22:32.238693Z","iopub.status.idle":"2026-01-14T08:22:57.546462Z","shell.execute_reply.started":"2026-01-14T08:22:32.238660Z","shell.execute_reply":"2026-01-14T08:22:57.545752Z"}},"outputs":[{"name":"stdout","text":"Classification Report:\n              precision    recall  f1-score   support\n\n     Class 0       0.98      0.04      0.07      5000\n     Class 1       0.11      0.99      0.20       600\n\n    accuracy                           0.14      5600\n   macro avg       0.55      0.52      0.13      5600\nweighted avg       0.89      0.14      0.08      5600\n\nAUC-ROC: 0.7381\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"strict_dataset, strict_loader, _, _ = load_train_test(\n    f\"{DATASET_DIR}/{DATASET_NAME}/BMAD-AD_{DATASET_NAME}_valid.npz\", device=DEVICE, batch_size=100, test_size=0\n)\n\n# Test set evaluation\nevaluate_model(final_detector, strict_loader, DEVICE, threshold=0.3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:22:57.547315Z","iopub.execute_input":"2026-01-14T08:22:57.547627Z","iopub.status.idle":"2026-01-14T08:23:09.094202Z","shell.execute_reply.started":"2026-01-14T08:22:57.547584Z","shell.execute_reply":"2026-01-14T08:23:09.093485Z"}},"outputs":[{"name":"stdout","text":"Classification Report:\n              precision    recall  f1-score   support\n\n     Class 0       1.00      0.04      0.09      1250\n     Class 1       0.11      1.00      0.20       150\n\n    accuracy                           0.15      1400\n   macro avg       0.56      0.52      0.14      1400\nweighted avg       0.90      0.15      0.10      1400\n\nAUC-ROC: 0.7490\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# # import matplotlib.pyplot as plt\n# # from torchvision import transforms\n# # from torch.utils.data import TensorDatasetss\n\n# def preview_images(loader, model=None, device='cpu', num_images=5):\n#     for images, labels in loader:\n#         images = images[:num_images]\n#         labels = labels[:num_images]\n\n#         if model is None:\n#             # Chỉ hiển thị ảnh\n#             imgs_np = images.cpu().numpy()\n#             labels_np = labels.cpu().numpy()\n#             imgs_np = np.clip(imgs_np, 0, 1)\n#             n = imgs_np.shape[0]\n#             fig, axes = plt.subplots(1, n, figsize=(3*n, 3))\n#             if n == 1:\n#                 axes = [axes]\n#             for j in range(n):\n#                 label = int(labels_np[j])\n#                 axes[j].imshow(imgs_np[j].transpose(1, 2, 0))\n#                 axes[j].set_title(f'[{label}] Image {j}')\n#                 axes[j].axis('off')\n#             plt.tight_layout()\n#             plt.show()\n#         else:\n#             # Hiển thị Original vs Reconstructed\n#             model.eval()\n#             with torch.no_grad():\n#                 images = images.to(device)\n#                 labels = labels.to(device)\n#                 reconstructed, _, _ = model(images, labels.unsqueeze(1))\n\n#             orig = images.cpu().numpy()\n#             recon = reconstructed.cpu().numpy()\n#             labels_np = labels.cpu().numpy()\n#             orig = np.clip(orig, 0, 1)\n#             recon = np.clip(recon, 0, 1)\n\n#             n = orig.shape[0]\n#             fig, axes = plt.subplots(2, n, figsize=(4*n, 8))\n#             if n == 1:\n#                 axes = axes.reshape(2, 1)\n\n#             for j in range(n):\n#                 label = int(labels_np[j])\n#                 axes[0, j].imshow(orig[j].transpose(1, 2, 0))\n#                 axes[0, j].set_title(f'[{label}] Org-{j}')\n#                 axes[0, j].axis('off')\n\n#                 axes[1, j].imshow(recon[j].transpose(1, 2, 0))\n#                 axes[1, j].set_title(f'[{label}] Recon-{j}')\n#                 axes[1, j].axis('off')\n#             plt.tight_layout()\n#             plt.show()\n\n#         break  # chỉ lấy 1 batch\n\n# def load_data(data, batch_num=64, is_shuffle=True):\n#     images = data['images']  # shape: (N, H, W, C)\n#     labels = data['labels']\n\n#     # Define transform\n#     transform = transforms.Compose([\n#         transforms.ToPILImage(),            # Chuyển numpy array -> PIL Image\n#         transforms.Resize((256, 256)),      # Resize về 256x256\n#         transforms.ToTensor()               # Chuyển PIL Image -> Tensor [0,1]\n#     ])\n\n#     # Áp dụng transform cho từng ảnh\n#     X = torch.stack([transform(img) for img in images])\n\n#     # Chuyển labels sang tensor\n#     y = torch.tensor(labels, dtype=torch.float32)\n\n#     # Tạo dataset & dataloader\n#     dataset = TensorDataset(X, y)\n#     loader = DataLoader(dataset, batch_size=batch_num, shuffle=is_shuffle)\n\n#     return loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:23:09.095126Z","iopub.execute_input":"2026-01-14T08:23:09.095422Z","iopub.status.idle":"2026-01-14T08:23:09.100607Z","shell.execute_reply.started":"2026-01-14T08:23:09.095399Z","shell.execute_reply":"2026-01-14T08:23:09.099496Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# syn_data = np.load(f\"/kaggle/working/synthetics/syn_ep{len(os.listdir(\"/kaggle/working/synthetics\"))}.npz\", allow_pickle=True)\n# syn_loader = load_data(syn_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:23:09.101581Z","iopub.execute_input":"2026-01-14T08:23:09.101835Z","iopub.status.idle":"2026-01-14T08:23:09.115762Z","shell.execute_reply.started":"2026-01-14T08:23:09.101804Z","shell.execute_reply":"2026-01-14T08:23:09.115256Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# preview_images(syn_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:23:09.116748Z","iopub.execute_input":"2026-01-14T08:23:09.117011Z","iopub.status.idle":"2026-01-14T08:23:09.126918Z","shell.execute_reply.started":"2026-01-14T08:23:09.116982Z","shell.execute_reply":"2026-01-14T08:23:09.126159Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# t_data = np.load(f\"{DATASET_DIR}/{DATASET_NAME}/BMAD-AD_{DATASET_NAME}_valid.npz\", allow_pickle=True)\n# t_loader = load_data(t_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:23:09.127793Z","iopub.execute_input":"2026-01-14T08:23:09.128153Z","iopub.status.idle":"2026-01-14T08:23:09.140408Z","shell.execute_reply.started":"2026-01-14T08:23:09.128121Z","shell.execute_reply":"2026-01-14T08:23:09.139704Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# preview_images(t_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:23:09.141318Z","iopub.execute_input":"2026-01-14T08:23:09.141673Z","iopub.status.idle":"2026-01-14T08:23:09.164883Z","shell.execute_reply.started":"2026-01-14T08:23:09.141640Z","shell.execute_reply":"2026-01-14T08:23:09.164216Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}