{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14276737,"sourceType":"datasetVersion","datasetId":9111559}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport random\nimport math\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, roc_auc_score\nfrom torchvision import models\nimport torch.optim as optim","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:20:12.095593Z","iopub.execute_input":"2026-01-14T08:20:12.095843Z","iopub.status.idle":"2026-01-14T08:20:20.571627Z","shell.execute_reply.started":"2026-01-14T08:20:12.095807Z","shell.execute_reply":"2026-01-14T08:20:20.571064Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"def set_seed(seed=42):\n    # Python\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n\n    # NumPy\n    np.random.seed(seed)\n\n    # PyTorch\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\nset_seed(11)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:20:20.573418Z","iopub.execute_input":"2026-01-14T08:20:20.573805Z","iopub.status.idle":"2026-01-14T08:20:20.583638Z","shell.execute_reply.started":"2026-01-14T08:20:20.573780Z","shell.execute_reply":"2026-01-14T08:20:20.582906Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# =========================================================\n# ENV\n# =========================================================\nclass LatentSpaceEnv:\n    def __init__(self, device=\"cpu\", max_steps=10, target_sim=0.7):\n        self.device = device\n        self.max_steps = max_steps\n        self.target_sim = target_sim\n\n        self.X = None\n        self.Y = None\n\n    # ================= DATA =================\n    def set_data(self, X, Y):\n        self.X = X.to(self.device)\n        self.Y = Y.to(self.device)\n\n        self.pos_idx = (self.Y == 1).nonzero(as_tuple=True)[0]\n        self.neg_idx = (self.Y == 0).nonzero(as_tuple=True)[0]\n\n        assert len(self.pos_idx) > 0, \"No label=1 samples\"\n\n    # ================= RESET =================\n    def reset(self):\n        self.current_idx = self.pos_idx[\n            torch.randint(len(self.pos_idx), (1,))\n        ].item()\n\n        self.x_orig = self.X[self.current_idx]\n        self.x_current = self.x_orig.clone()\n\n        self.step_count = 0\n        return self.get_state()\n\n    # ================= STATE =================\n    def get_state(self):\n        return self.x_current.clone()\n\n    # ================= REWARD =================\n    def compute_reward(self, x_prime):\n        pos_samples = self.X[self.pos_idx]\n        neg_samples = self.X[self.neg_idx]\n\n        mean_pos = torch.mean(torch.norm(x_prime - pos_samples, dim=1))\n        mean_neg = torch.mean(torch.norm(x_prime - neg_samples, dim=1))\n        class_margin = torch.sigmoid(mean_neg - mean_pos)\n\n        cos_pos = F.cosine_similarity(\n            x_prime.unsqueeze(0), pos_samples, dim=1\n        )\n        max_sim = torch.max(cos_pos)\n        diversity_penalty = torch.relu(max_sim - 0.95)\n\n        sim_orig = F.cosine_similarity(x_prime, self.x_orig, dim=0)\n        target_reward = torch.exp(\n            -((sim_orig - self.target_sim) ** 2) / 0.02\n        )\n\n        reward = (\n            0.5 * class_margin\n            + 0.4 * target_reward\n            - 0.6 * diversity_penalty\n        )\n\n        return reward  # scalar\n\n    # ================= STEP =================\n    def step(self, action):\n        self.step_count += 1\n\n        x_prime = self.x_current + action\n        reward = self.compute_reward(x_prime)\n\n        self.x_current = x_prime.detach()\n        done = self.step_count >= self.max_steps\n\n        return self.get_state(), reward, done\n\n\n# =========================================================\n# ACTOR / CRITIC\n# =========================================================\nclass Actor(nn.Module):\n    def __init__(self, latent_dim, hidden=256):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(latent_dim, hidden),\n            nn.ReLU(),\n            nn.Linear(hidden, hidden),\n            nn.ReLU()\n        )\n        self.mu = nn.Linear(hidden, latent_dim)\n        self.logstd = nn.Parameter(torch.zeros(latent_dim))\n\n    def forward(self, x):\n        h = self.net(x)\n        std = self.logstd.exp()\n        return self.mu(h), std\n\n\nclass Critic(nn.Module):\n    def __init__(self, latent_dim, hidden=256):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(latent_dim, hidden),\n            nn.ReLU(),\n            nn.Linear(hidden, hidden),\n            nn.ReLU(),\n            nn.Linear(hidden, 1)\n        )\n\n    def forward(self, x):\n        return self.net(x).squeeze(-1)\n\n\n# =========================================================\n# ROLLOUT BUFFER\n# =========================================================\nclass RolloutBuffer:\n    def __init__(self):\n        self.clear()\n\n    def clear(self):\n        self.states = []\n        self.actions = []\n        self.logps = []\n        self.rewards = []\n        self.values = []\n        self.dones = []\n\n    def add(self, s, a, logp, r, v, d):\n        self.states.append(s)\n        self.actions.append(a)\n        self.logps.append(logp)\n        self.rewards.append(r)\n        self.values.append(v)\n        self.dones.append(d)\n\n\n# =========================================================\n# PPO\n# =========================================================\nclass PPO:\n    def __init__(\n        self,\n        actor,\n        critic,\n        env,\n        lr=3e-4,\n        gamma=0.99,\n        lam=0.95,\n        clip_eps=0.2,\n        train_epochs=5,\n        device=\"cpu\"\n    ):\n        self.actor = actor.to(device)\n        self.critic = critic.to(device)\n        self.env = env\n\n        self.gamma = gamma\n        self.lam = lam\n        self.clip_eps = clip_eps\n        self.train_epochs = train_epochs\n        self.device = device\n\n        self.opt_actor = optim.Adam(self.actor.parameters(), lr=lr)\n        self.opt_critic = optim.Adam(self.critic.parameters(), lr=lr)\n\n        self.buffer = RolloutBuffer()\n\n    # ================= ROLLOUT =================\n    def collect_trajectories(self, num_episodes=4):\n        self.buffer.clear()\n        episode_rewards = []\n\n        for _ in range(num_episodes):\n            state = self.env.reset()\n            ep_reward = 0.0\n\n            for _ in range(self.env.max_steps):\n                state = state.to(self.device)\n\n                mu, std = self.actor(state)\n                dist = torch.distributions.Normal(mu, std)\n\n                action = dist.sample()\n                logp = dist.log_prob(action).sum(dim=-1)\n\n                value = self.critic(state)\n\n                next_state, reward, done = self.env.step(action)\n\n                self.buffer.add(\n                    state.detach(),\n                    action.detach(),\n                    logp.detach(),\n                    reward.detach(),\n                    value.detach(),\n                    done\n                )\n\n                state = next_state\n                ep_reward += reward.item()\n\n                if done:\n                    break\n\n            episode_rewards.append(ep_reward)\n\n        return {\"avg_reward\": float(np.mean(episode_rewards))}\n\n    # ================= GAE =================\n    def compute_gae(self):\n        rewards = self.buffer.rewards\n        values = self.buffer.values\n        dones = self.buffer.dones\n\n        advantages = []\n        returns = []\n\n        gae = 0\n        next_value = 0\n\n        for t in reversed(range(len(rewards))):\n            mask = 1.0 - float(dones[t])\n            delta = rewards[t] + self.gamma * next_value * mask - values[t]\n            gae = delta + self.gamma * self.lam * mask * gae\n\n            advantages.insert(0, gae)\n            returns.insert(0, gae + values[t])\n\n            next_value = values[t]\n\n        return torch.stack(advantages), torch.stack(returns)\n\n    # ================= UPDATE =================\n    def learn(self):\n        states = torch.stack(self.buffer.states)\n        actions = torch.stack(self.buffer.actions)\n        old_logps = torch.stack(self.buffer.logps)\n\n        advantages, returns = self.compute_gae()\n        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n\n        for _ in range(self.train_epochs):\n            mu, std = self.actor(states)\n            dist = torch.distributions.Normal(mu, std)\n            logps = dist.log_prob(actions).sum(dim=-1)\n\n            ratio = torch.exp(logps - old_logps)\n\n            surr1 = ratio * advantages\n            surr2 = torch.clamp(\n                ratio, 1 - self.clip_eps, 1 + self.clip_eps\n            ) * advantages\n\n            actor_loss = -torch.min(surr1, surr2).mean()\n\n            values = self.critic(states)\n            critic_loss = F.mse_loss(values, returns)\n\n            self.opt_actor.zero_grad()\n            actor_loss.backward()\n            self.opt_actor.step()\n\n            self.opt_critic.zero_grad()\n            critic_loss.backward()\n            self.opt_critic.step()\n\n        avg_reward = torch.stack(self.buffer.rewards).mean().item()\n        return actor_loss.item(), critic_loss.item(), avg_reward\n\n    # ================= INFER =================\n    @torch.no_grad()\n    def infer(self, z0, steps=None, step_scale=0.1):\n        self.actor.eval()\n\n        if steps is None:\n            steps = self.env.max_steps\n\n        z = z0.clone().to(self.device)\n\n        for _ in range(steps):\n            mu, _ = self.actor(z)\n            z = z + step_scale * torch.tanh(mu)\n\n        return z\n\n\n\n# =========================================================\n# OFFLINE REFINEMENT\n# =========================================================\n@torch.no_grad()\ndef refine_latent_offline(actor, x0, steps=8, step_scale=0.1):\n    x = x0.clone()\n    for _ in range(steps):\n        mu, _ = actor(x)\n        x = x + step_scale * torch.tanh(mu)\n    return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:20:20.584396Z","iopub.execute_input":"2026-01-14T08:20:20.584595Z","iopub.status.idle":"2026-01-14T08:20:20.610828Z","shell.execute_reply.started":"2026-01-14T08:20:20.584575Z","shell.execute_reply":"2026-01-14T08:20:20.610257Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class NPZDataset(Dataset):\n    def __init__(self, path, indices=None, resize=256, device=\"cpu\"):\n        self.data = np.load(path, allow_pickle=True, mmap_mode=\"r\")\n        self.images = self.data[\"images\"]\n        self.labels = self.data[\"labels\"]\n\n        if indices is None:\n            self.indices = np.arange(len(self.labels))\n        else:\n            self.indices = indices\n\n        self.resize = resize\n        self.device = device\n\n    def __len__(self):\n        return len(self.indices)\n\n    def __getitem__(self, idx):\n        i = self.indices[idx]\n        img = torch.tensor(self.images[i]).float()\n        lbl = torch.tensor(self.labels[i]).float()\n\n        img = img.permute(2,0,1)\n        if img.max() > 1:\n            img = img / 255.\n\n        img = F.interpolate(\n            img.unsqueeze(0),\n            size=(self.resize, self.resize),\n            mode=\"bilinear\",\n            align_corners=False\n        ).squeeze(0)\n\n        return img.to(self.device), lbl.to(self.device)\n\nclass SyntheticDataset(Dataset):\n    def __init__(self, path, device):\n        self.data = np.load(path, mmap_mode=\"r\")\n        self.images = self.data[\"images\"]\n        self.labels = self.data[\"labels\"]\n        self.device = device\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        img = torch.from_numpy(self.images[idx]).float().permute(2,0,1)\n        lbl = torch.tensor(self.labels[idx], dtype=torch.float32)\n        return img.to(self.device), lbl.to(self.device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:20:20.611600Z","iopub.execute_input":"2026-01-14T08:20:20.611836Z","iopub.status.idle":"2026-01-14T08:20:20.626072Z","shell.execute_reply.started":"2026-01-14T08:20:20.611816Z","shell.execute_reply":"2026-01-14T08:20:20.625410Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def load_train_test(path, device=\"cpu\", batch_size=32, resize=256, test_size=0.6):\n    dataset_full = NPZDataset(path, device=device, resize=resize)\n    indices = np.arange(len(dataset_full.labels))\n\n    # Nếu test_size = 0 → không split\n    if test_size == 0:\n        train_idx = indices\n        test_idx = []\n    else:\n        train_idx, test_idx = train_test_split(\n            indices,\n            test_size=test_size,\n            stratify=dataset_full.labels,\n            random_state=42\n        )\n\n    train_dataset = NPZDataset(path, indices=train_idx, device=device, resize=resize)\n    test_dataset  = NPZDataset(path, indices=test_idx,  device=device, resize=resize) if len(test_idx) > 0 else None\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader  = DataLoader(test_dataset,  batch_size=batch_size) if test_dataset is not None else None\n\n    return train_dataset, train_loader, test_dataset, test_loader\n\ndef count_classes(dataset):\n    count0 = 0\n    count1 = 0\n\n    for i in range(len(dataset)):\n        lbl = dataset[i][1]    # dataset[i] → (image, label)\n        lbl = int(lbl.item())  # convert to 0 hoặc 1\n\n        if lbl == 0:\n            count0 += 1\n        else:\n            count1 += 1\n    return count0, count1\n\ndef evaluate_model(model, loader, device, threshold=0.5):\n    model.eval()\n    all_preds, all_labels = [], []\n\n    with torch.no_grad():\n        for x_batch, y_batch in loader:\n            x_batch = x_batch.to(device)\n            y_batch = y_batch.to(device)\n            y_pred = model(x_batch).squeeze()\n            all_preds.append(y_pred.cpu())\n            all_labels.append(y_batch.cpu())\n\n    preds = torch.cat(all_preds).numpy()\n    labels = torch.cat(all_labels).numpy()\n\n\n    binary_pred = (preds > threshold).astype(int)\n\n    print(\"Classification Report:\")\n    print(classification_report(labels, binary_pred, labels=[0,1], target_names=[\"Class 0\",\"Class 1\"]))\n\n    if len(set(labels)) > 1:\n        auc = roc_auc_score(labels, preds)\n        print(f\"AUC-ROC: {auc:.4f}\")\n    else:\n        print(\"AUC-ROC: Undefined (only one class present)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:20:20.627003Z","iopub.execute_input":"2026-01-14T08:20:20.627334Z","iopub.status.idle":"2026-01-14T08:20:20.643463Z","shell.execute_reply.started":"2026-01-14T08:20:20.627313Z","shell.execute_reply":"2026-01-14T08:20:20.642892Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class ResNetEmbedder(nn.Module):\n    def __init__(self, output_dim=128, pretrained=True):\n        super().__init__()\n        self.backbone = models.resnet50(pretrained=pretrained)\n        self.backbone.fc = nn.Identity()\n        self.dim_adjuster = nn.Linear(2048, output_dim)\n        self.output_dim = output_dim\n\n    def forward(self, images):\n        features = self.backbone(images)\n        features = features.squeeze()\n        if features.dim() == 1:\n            features = features.unsqueeze(0)\n        features = self.dim_adjuster(features)\n        return features\n\nclass CNNAutoEncoder(nn.Module):\n    def __init__(self, latent_dim=128):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            nn.Conv2d(3,64,4,2,1), nn.BatchNorm2d(64), nn.ReLU(),\n            nn.Conv2d(64,128,4,2,1), nn.BatchNorm2d(128), nn.ReLU(),\n            nn.Conv2d(128,256,4,2,1), nn.BatchNorm2d(256), nn.ReLU(),\n            nn.Conv2d(256,512,4,2,1), nn.BatchNorm2d(512), nn.ReLU()\n        )\n        self.encoder2latent = nn.Linear(512*16*16+1, latent_dim)\n        self.latent2decoder = nn.Linear(latent_dim+1, 512*16*16)\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(512,256,4,2,1), nn.BatchNorm2d(256), nn.ReLU(),\n            nn.ConvTranspose2d(256,128,4,2,1), nn.BatchNorm2d(128), nn.ReLU(),\n            nn.ConvTranspose2d(128,64,4,2,1), nn.BatchNorm2d(64), nn.ReLU(),\n            nn.ConvTranspose2d(64,3,4,2,1), nn.Sigmoid()\n        )\n\n    def encode(self, x, y):\n        h = self.encoder(x)\n        flat = h.view(x.size(0), -1)\n        xy = torch.cat([flat, y], dim=1)\n        z = self.encoder2latent(xy)\n        return z\n\n    def decode(self, z, y):\n        zy = torch.cat([z, y], dim=1)\n        flat = self.latent2decoder(zy)\n        h = flat.view(z.size(0),512,16,16)\n        x_recon = self.decoder(h)\n        return x_recon\n\n    def forward(self, x, y):\n        z = self.encode(x, y)\n        return self.decode(z, y)\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0,max_len).unsqueeze(1).float()\n        div_term = torch.exp(torch.arange(0,d_model,2).float()*(-np.log(10000.0)/d_model))\n        pe[:,0::2] = torch.sin(position*div_term)\n        pe[:,1::2] = torch.cos(position*div_term)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self,x):\n        L = x.size(1)\n        return x + self.pe[:,:L,:]\n\nclass ViTransformerDetector(nn.Module):\n    def __init__(self, embedder, d_model=128, nhead=8, num_layers=2, dim_feedforward=256, dropout=0.1):\n        super().__init__()\n        self.embedder = embedder\n        self.embedding_dim = embedder.output_dim\n        self.projection = nn.Linear(self.embedding_dim,d_model)\n        self.positional_encoding = PositionalEncoding(d_model)\n        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,\n                                                   dim_feedforward=dim_feedforward,\n                                                   dropout=dropout, batch_first=True)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.fc = nn.Sequential(\n            nn.Linear(d_model,128), nn.ReLU(), nn.Dropout(dropout),\n            nn.Linear(128,1), nn.Sigmoid()\n        )\n\n    def forward(self,x):\n        emb = self.embedder(x).view(x.size(0),1,-1)\n        emb = self.projection(emb)\n        emb = self.positional_encoding(emb)\n        h = self.transformer_encoder(emb)\n        pooled = h.mean(dim=1)\n        return self.fc(pooled).squeeze(1)\n\nclass EarlyStopping:\n    def __init__(self, patience=5, min_delta=1e-4):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.best_loss = float(\"inf\")\n        self.early_stop = False\n\n    def step(self, current_loss):\n        if current_loss < self.best_loss - self.min_delta:\n            self.best_loss = current_loss\n            self.counter = 0\n        else:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True\n\n    def reset(self):\n        self.counter = 0\n        self.best_loss = float(\"inf\")\n        self.early_stop = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:20:20.644383Z","iopub.execute_input":"2026-01-14T08:20:20.645047Z","iopub.status.idle":"2026-01-14T08:20:20.662472Z","shell.execute_reply.started":"2026-01-14T08:20:20.645024Z","shell.execute_reply":"2026-01-14T08:20:20.661893Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def train_cnn_ae(model, loader, opt, device):\n    model.train()\n    total = 0\n    for x,y in loader:\n        x = x.to(device)\n        y = y.unsqueeze(1).to(device)\n        recon = model.decode(model.encode(x,y), y)\n        loss = torch.mean(torch.abs(x - recon))\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n        total += loss.item()\n    return total/len(loader)\n\ndef train_detector(model, loader, opt, criterion, device):\n    model.train()\n    total = 0\n    for x,y in loader:\n        x=x.to(device); y=y.to(device)\n        out = model(x)\n        loss = criterion(out, y)\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n        total += loss.item()\n    return total/len(loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:20:20.664343Z","iopub.execute_input":"2026-01-14T08:20:20.664680Z","iopub.status.idle":"2026-01-14T08:20:20.679480Z","shell.execute_reply.started":"2026-01-14T08:20:20.664657Z","shell.execute_reply":"2026-01-14T08:20:20.678987Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def One_Step_To_Feasible_Action(generator, detector, x_orig, device,\n                                previously_generated=None, alpha=1.0,\n                                lambda_div=0.1, lr=0.001, steps=50):\n    generator.eval()\n    detector.eval()\n    if previously_generated is None:\n        previously_generated = []\n\n    x_orig = x_orig.to(device).unsqueeze(0)\n    y_class1 = torch.full((1,1),0.8,device=device)\n\n    with torch.no_grad():\n        z = generator.encode(x_orig, y_class1)\n    z = z.clone().detach().requires_grad_(True)\n\n    optimizer_z = torch.optim.Adam([z], lr=lr)\n\n    for step in range(steps):\n        optimizer_z.zero_grad()\n        x_syn = generator.decode(z, y_class1)\n        prob_class1 = detector(x_syn)\n\n        if len(previously_generated) > 0:\n            old = torch.stack(previously_generated).to(device)\n            diff = x_syn.unsqueeze(1)-old.unsqueeze(0)\n            dist_min = diff.norm(p=2,dim=(2,3,4)).min(dim=1).values\n            diversity_term = torch.exp(-alpha*dist_min).mean()\n        else:\n            diversity_term = torch.tensor(0.0, device=device)\n\n        reward = prob_class1.mean() + lambda_div*diversity_term\n        reward.backward()\n        optimizer_z.step()\n\n    with torch.no_grad():\n        x_adv = generator.decode(z, y_class1).detach().cpu()\n\n    return x_adv\n\ndef Gen_with_PPO(\n    actor,\n    generator,\n    x_orig,\n    device,\n    episodes,\n    steps=8\n):\n    actor.eval()\n    generator.eval()\n\n    # ===== y_class1 GIỮ NGUYÊN =====\n    conf = 0.8 + 0.1 / (1 + math.exp(-0.1 * (episodes - 10)))\n    y_class1 = torch.full(\n        (1, 1),\n        random.uniform(max(conf - 0.02, 0.8), conf),\n        device=device\n    )\n\n    with torch.no_grad():\n        z0 = generator.encode(\n            x_orig.unsqueeze(0).to(device),\n            y_class1\n        )\n\n    # ✅ ĐÚNG TÊN HÀM\n    z_adv = refine_latent_offline(\n        actor,\n        z0.squeeze(0),\n        steps=steps\n    )\n\n    if not check_valid_variant(z_adv, z0.squeeze(0)):\n        return None\n\n    with torch.no_grad():\n        x_synthetic = generator.decode(\n            z_adv.unsqueeze(0),\n            y_class1\n        )\n\n    return x_synthetic\n\n\ndef check_valid_variant(x, x_prime, threshold=0.3):\n    # Flatten nếu có batch dimension\n    if x.dim() > 1:\n        x_flat = x.view(x.size(0), -1)\n        x_prime_flat = x_prime.view(x_prime.size(0), -1)\n        cos_sim = torch.nn.functional.cosine_similarity(x_flat, x_prime_flat, dim=1)\n    else:\n        cos_sim = torch.nn.functional.cosine_similarity(x, x_prime, dim=0)\n    \n    cos_dist = 1 - cos_sim\n    \n    # Trả về boolean value duy nhất\n    # Nếu có nhiều samples, kiểm tra xem TẤT CẢ có valid không\n    is_valid = (cos_dist < threshold).all()\n    \n    return is_valid.item()  # Convert tensor to Python bool\n\ndef load_z_space(generator, loader, device):\n    generator.eval()\n\n    all_z = []\n    all_y = []\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device)\n            y = y.unsqueeze(1).to(device)\n\n            z = generator.encode(x, y)   # (batch, z_dim)\n\n            all_z.append(z.cpu())\n            all_y.append(y.cpu())\n\n    # Ghép toàn bộ batch lại\n    all_z = torch.cat(all_z, dim=0)\n    all_y = torch.cat(all_y, dim=0)\n\n    return all_z, all_y\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:20:20.680142Z","iopub.execute_input":"2026-01-14T08:20:20.680350Z","iopub.status.idle":"2026-01-14T08:20:20.699920Z","shell.execute_reply.started":"2026-01-14T08:20:20.680321Z","shell.execute_reply":"2026-01-14T08:20:20.699342Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nDATASET_DIR = \"/kaggle/input/anomaly-dataset-npz/npz-dataset/BMAD-AD\"\n# DATASET_NAME = \"RESC\"\nDATASET_NAME = \"camelyon16_256\"\n# DATASET_NAME = \"hist_DIY\"\nDATASET_PATH = f\"{DATASET_DIR}/{DATASET_NAME}/BMAD-AD_{DATASET_NAME}_train.npz\"\nSYNTHETIC_DIR = f\"synthetics/\"\nos.makedirs(SYNTHETIC_DIR, exist_ok=True)\nMODEL_DIR = f\"{DATASET_NAME}_models/\"\nos.makedirs(MODEL_DIR, exist_ok=True)\nPATIENTCE = 5\nBATCH_SIZE = 100\nLATENT_DIM = 64\nREFINE_STEP = 10\n\ntrain_dataset, train_loader, test_dataset, test_loader = load_train_test(\n    DATASET_PATH, device=DEVICE, batch_size=BATCH_SIZE, resize=256\n)\n\n# # ===== ENV =====\n# env = LatentSpaceEnv(\n#     device=DEVICE,\n#     max_steps=REFINE_STEP\n# )\n\n# # ===== ACTOR / CRITIC =====\n# actor = Actor(\n#     latent_dim=LATENT_DIM,\n#     hidden=256\n# )\n\n# critic = Critic(\n#     latent_dim=LATENT_DIM,\n#     hidden=256\n# )\n\n# # ===== PPO =====\n# ppo = PPO(\n#     actor=actor,\n#     critic=critic,\n#     env=env,\n#     lr=3e-4,\n#     gamma=0.99,\n#     lam=0.95,\n#     clip_eps=0.2,\n#     device=DEVICE\n# )\n\n# generator = CNNAutoEncoder(latent_dim=LATENT_DIM).to(DEVICE)\nembedder = ResNetEmbedder(output_dim=128).to(DEVICE)\n# detector = ViTransformerDetector(embedder=embedder).to(DEVICE)\nearly_stopper = EarlyStopping(patience=PATIENTCE)\n\n# optimizer_g = torch.optim.Adam(generator.parameters(), lr=1e-4)\n# optimizer_d = torch.optim.Adam(detector.parameters(), lr=1e-4)\n# criterion = nn.BCELoss()\n\n# synthetic_files = []\n# num_epochs_generator = 100\n# num_epochs_detector = 100\n# num_episodes = 100\n# num_steps_ppo = 100\n# num_gen_data = 100\n# combined_dataset = train_dataset\n\n# for ep in range(num_episodes):\n#     print(f\"\\n================ EPISODE {ep+1}/{num_episodes} ================\")\n#     total_class0, total_class1 = count_classes(combined_dataset)\n\n#     # 2️⃣ Print\n#     print(f\"[Episode {ep+1}] Class count → 0: {total_class0}, 1: {total_class1}\")\n\n#     # 3️⃣ Auto-stop if balanced\n#     if total_class1 >= total_class0:\n#         print(\"Dataset is now balanced. Stopping synthetic generation.\")\n#         break\n\n#     # TRAIN GENERATOR\n#     early_stopper.reset()\n#     for epoch in range(num_epochs_generator):\n#         loss_g = train_cnn_ae(generator, train_loader, optimizer_g, DEVICE)\n#         if (epoch + 1) % 10 == 0:\n#             print(f\"[GENERATOR] Epoch {epoch+1}/{num_epochs_generator}, Loss={loss_g:.4f}\")\n#         early_stopper.step(loss_g)\n#         if early_stopper.early_stop:\n#             print(f\"[GENERATOR] Early stopping at epoch {epoch+1}\")\n#             print(f\"[GENERATOR] Epoch {epoch+1}/{num_epochs_generator}, Loss={loss_g:.4f}\")\n#             break\n\n#     torch.save(generator.state_dict(), f\"{MODEL_DIR}/{DATASET_NAME}_generator.pth\")\n\n#     # 4️⃣ Train detector\n    \n#     # early_stopper.reset()\n#     # for epoch in range(num_epochs_detector):\n#     #     loss_d = train_detector(detector, train_loader, optimizer_d, criterion, DEVICE)\n#     #     if (epoch + 1) % 10 == 0:\n#     #         print(f\"[DETECTOR] Epoch {epoch+1}/{num_epochs_detector}, Loss={loss_d:.4f}\")\n#     #     early_stopper.step(loss_d)\n#     #     if early_stopper.early_stop:\n#     #         print(f\"[DETECTOR] Early stopping at epoch {epoch+1}\")\n#     #         print(f\"[DETECTOR] Epoch {epoch+1}/{num_epochs_detector}, Loss={loss_d:.4f}\")\n#     #         break\n\n#     # torch.save(detector.state_dict(), f\"{MODEL_DIR}/{DATASET_NAME}_detector.pth\")\n\n\n#     # 5️⃣ Generate synthetic samples (class 1)\n#     synthetic_samples = []\n#     idx_class1 = [i for i, idx_lbl in enumerate(train_dataset.indices) if train_dataset.labels[idx_lbl] == 1]\n\n#     ppo_gen_count = 0\n#     for _ in range(num_gen_data):\n#         if not idx_class1:\n#             break\n#         rand_idx = np.random.choice(idx_class1)\n#         x_orig, _ = train_dataset[rand_idx]\n#         x_adv = None\n#         if ep != 0:\n#             x_adv = Gen_with_PPO(actor, generator, x_orig, DEVICE, ep+1, REFINE_STEP)\n#         if x_adv is None:\n#             x_adv = One_Step_To_Feasible_Action(generator, detector, x_orig, DEVICE)\n#         else:\n#             ppo_gen_count += 1\n#         synthetic_samples.append(x_adv.to(DEVICE))\n\n#     ppo_gen_rate  = (ppo_gen_count / num_gen_data) * 100\n#     print(f\"{ppo_gen_rate:.2f}% of data generated by PPO strategy\")\n    \n#     syn_tensor = torch.cat(synthetic_samples)            # (N,3,H,W)\n#     syn_tensor_hwc = syn_tensor.permute(0,2,3,1).cpu().numpy()   # (N,H,W,3)  <-- HWC\n#     syn_labels = np.ones(len(synthetic_samples), dtype=np.float32)\n\n#     syn_path = f\"{SYNTHETIC_DIR}/syn_ep{ep+1}.npz\"\n#     np.savez_compressed(syn_path, images=syn_tensor_hwc, labels=syn_labels)\n#     synthetic_files.append(syn_path)\n\n#     print(f\"[EP {ep+1}] Generated synthetic saved:\", syn_path)\n\n#     synthetic_subsets = []\n#     for f in synthetic_files:\n#         synthetic_subsets.append(SyntheticDataset(f, DEVICE))\n\n#     synthetic_full = ConcatDataset(synthetic_subsets)\n#     combined_dataset = ConcatDataset([train_dataset, synthetic_full])\n\n#     train_loader = DataLoader(combined_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\n#     Z_ppo, Y_ppo = load_z_space(generator, train_loader, DEVICE)\n#     # env.set_models(generator, detector)\n#     env.set_data(Z_ppo, Y_ppo)\n#     # ===== TRAIN PPO =====\n#     early_stopper.reset()\n    \n#     for step in range(num_steps_ppo):\n    \n#         rollout_stats = ppo.collect_trajectories(\n#             num_episodes=4   # ✅ FIX CỨNG, KHÔNG DÙNG env.batch_size\n#         )\n    \n#         a_loss, c_loss, avg_reward = ppo.learn()\n    \n#         if (step + 1) % 1 == 0:\n#             print(\n#                 f\"Step {step:03d} | \"\n#                 f\"Actor Loss = {a_loss:.4f}, \"\n#                 f\"Critic Loss = {c_loss:.4f}, \"\n#                 f\"AvgReward = {avg_reward:.4f}\"\n#             )\n    \n#         early_stopper.step(-avg_reward)\n#         if early_stopper.early_stop:\n#             print(f\"[PPO] Early stopping at step {step+1}\")\n#             print(\n#                 f\"Step {step:03d} | \"\n#                 f\"Actor Loss = {a_loss:.4f}, \"\n#                 f\"Critic Loss = {c_loss:.4f}, \"\n#                 f\"AvgReward = {avg_reward:.4f}\"\n#             )\n#             break\n\n    \n#     print(f\"[EP {ep+1}] Train dataset expanded: now {len(combined_dataset)} samples\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:41:45.366858Z","iopub.execute_input":"2026-01-14T08:41:45.367492Z","iopub.status.idle":"2026-01-14T08:42:00.212249Z","shell.execute_reply.started":"2026-01-14T08:41:45.367460Z","shell.execute_reply":"2026-01-14T08:42:00.211664Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"final_detector = ViTransformerDetector(embedder=embedder).to(DEVICE)\noptimizer_final = torch.optim.Adam(final_detector.parameters(), lr=1e-3)\ncriterion = nn.BCELoss()\nnum_epochs_final = 100\nearly_stopper.reset()\n\nfor epoch in range(num_epochs_final):\n    loss = train_detector(final_detector, train_loader, optimizer_final, criterion, DEVICE)\n    print(f\"[FINAL DETECTOR] Epoch {epoch+1}/{num_epochs_final}, Loss={loss:.4f}\")\n\n    # Check early stopping\n    early_stopper.step(loss)\n\n    if early_stopper.early_stop:\n        print(f\"[FINAL DETECTOR] Early stopping at epoch {epoch+1}\")\n        print(f\"[FINAL DETECTOR] Epoch {epoch+1}/{num_epochs_final}, Loss={loss:.4f}\")\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:42:00.213414Z","iopub.execute_input":"2026-01-14T08:42:00.213648Z","iopub.status.idle":"2026-01-14T08:50:14.846229Z","shell.execute_reply.started":"2026-01-14T08:42:00.213627Z","shell.execute_reply":"2026-01-14T08:50:14.845547Z"}},"outputs":[{"name":"stdout","text":"[FINAL DETECTOR] Epoch 1/100, Loss=0.3500\n[FINAL DETECTOR] Epoch 2/100, Loss=0.2403\n[FINAL DETECTOR] Epoch 3/100, Loss=0.1958\n[FINAL DETECTOR] Epoch 4/100, Loss=0.1515\n[FINAL DETECTOR] Epoch 5/100, Loss=0.1395\n[FINAL DETECTOR] Epoch 6/100, Loss=0.1290\n[FINAL DETECTOR] Epoch 7/100, Loss=0.1028\n[FINAL DETECTOR] Epoch 8/100, Loss=0.1229\n[FINAL DETECTOR] Epoch 9/100, Loss=0.1048\n[FINAL DETECTOR] Epoch 10/100, Loss=0.0830\n[FINAL DETECTOR] Epoch 11/100, Loss=0.0817\n[FINAL DETECTOR] Epoch 12/100, Loss=0.0814\n[FINAL DETECTOR] Epoch 13/100, Loss=0.0727\n[FINAL DETECTOR] Epoch 14/100, Loss=0.0972\n[FINAL DETECTOR] Epoch 15/100, Loss=0.0762\n[FINAL DETECTOR] Epoch 16/100, Loss=0.1042\n[FINAL DETECTOR] Epoch 17/100, Loss=0.0907\n[FINAL DETECTOR] Epoch 18/100, Loss=0.0580\n[FINAL DETECTOR] Epoch 19/100, Loss=0.0943\n[FINAL DETECTOR] Epoch 20/100, Loss=0.0529\n[FINAL DETECTOR] Epoch 21/100, Loss=0.0435\n[FINAL DETECTOR] Epoch 22/100, Loss=0.0545\n[FINAL DETECTOR] Epoch 23/100, Loss=0.0268\n[FINAL DETECTOR] Epoch 24/100, Loss=0.0254\n[FINAL DETECTOR] Epoch 25/100, Loss=0.0146\n[FINAL DETECTOR] Epoch 26/100, Loss=0.0245\n[FINAL DETECTOR] Epoch 27/100, Loss=0.0211\n[FINAL DETECTOR] Epoch 28/100, Loss=0.0372\n[FINAL DETECTOR] Epoch 29/100, Loss=0.0285\n[FINAL DETECTOR] Epoch 30/100, Loss=0.0312\n[FINAL DETECTOR] Early stopping at epoch 30\n[FINAL DETECTOR] Epoch 30/100, Loss=0.0312\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"evaluate_model(final_detector, test_loader, DEVICE, threshold=0.3)\n\n# Save final detector\ntorch.save(final_detector.state_dict(), f\"{MODEL_DIR}/{DATASET_NAME}_detector_final.pth\")\nprint(f\"Final detector saved to {MODEL_DIR}/{DATASET_NAME}_detector_final.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:50:14.847145Z","iopub.execute_input":"2026-01-14T08:50:14.847383Z","iopub.status.idle":"2026-01-14T08:50:25.382888Z","shell.execute_reply.started":"2026-01-14T08:50:14.847361Z","shell.execute_reply":"2026-01-14T08:50:25.382202Z"}},"outputs":[{"name":"stdout","text":"Classification Report:\n              precision    recall  f1-score   support\n\n     Class 0       0.95      0.97      0.96      2980\n     Class 1       0.79      0.70      0.74       534\n\n    accuracy                           0.93      3514\n   macro avg       0.87      0.83      0.85      3514\nweighted avg       0.92      0.93      0.93      3514\n\nAUC-ROC: 0.9043\nFinal detector saved to camelyon16_256_models//camelyon16_256_detector_final.pth\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# torch.save(actor.state_dict(), f\"{MODEL_DIR}/{DATASET_NAME}_actor.pth\")\n# print(f\"PPO Actor saved to {MODEL_DIR}/{DATASET_NAME}_actor.pth\")\n\n# torch.save(critic.state_dict(), f\"{MODEL_DIR}/{DATASET_NAME}_critic.pth\")\n# print(f\"PPO Critic saved to {MODEL_DIR}/{DATASET_NAME}_critic.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:50:25.384399Z","iopub.execute_input":"2026-01-14T08:50:25.384672Z","iopub.status.idle":"2026-01-14T08:50:25.388536Z","shell.execute_reply.started":"2026-01-14T08:50:25.384650Z","shell.execute_reply":"2026-01-14T08:50:25.387721Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# import shutil\n\n# synthetics_folder = \"/kaggle/working/synthetics\"\n# synthetics_zip = f\"{DATASET_NAME}_synthetics\"\n\n# shutil.make_archive(synthetics_zip, 'zip', synthetics_folder)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:50:25.389324Z","iopub.execute_input":"2026-01-14T08:50:25.389550Z","iopub.status.idle":"2026-01-14T08:50:25.398777Z","shell.execute_reply.started":"2026-01-14T08:50:25.389529Z","shell.execute_reply":"2026-01-14T08:50:25.398003Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"import shutil\n\nmodels_folder = f\"/kaggle/working/{DATASET_NAME}_models\"\nmodels_zip = f\"{DATASET_NAME}_models\"\n\nshutil.make_archive(models_zip, 'zip', models_folder)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:50:25.399766Z","iopub.execute_input":"2026-01-14T08:50:25.400022Z","iopub.status.idle":"2026-01-14T08:50:30.248218Z","shell.execute_reply.started":"2026-01-14T08:50:25.399994Z","shell.execute_reply":"2026-01-14T08:50:30.247465Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/camelyon16_256_models.zip'"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"strict_dataset, strict_loader, _, _ = load_train_test(\n    f\"{DATASET_DIR}/{DATASET_NAME}/BMAD-AD_{DATASET_NAME}_train.npz\", device=DEVICE, batch_size=100, test_size=0\n)\n\n# Test set evaluation\nevaluate_model(final_detector, strict_loader, DEVICE, threshold=0.3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:50:30.249252Z","iopub.execute_input":"2026-01-14T08:50:30.249628Z","iopub.status.idle":"2026-01-14T08:50:50.950421Z","shell.execute_reply.started":"2026-01-14T08:50:30.249598Z","shell.execute_reply":"2026-01-14T08:50:50.949646Z"}},"outputs":[{"name":"stdout","text":"Classification Report:\n              precision    recall  f1-score   support\n\n     Class 0       0.96      0.98      0.97      4966\n     Class 1       0.86      0.80      0.83       890\n\n    accuracy                           0.95      5856\n   macro avg       0.91      0.89      0.90      5856\nweighted avg       0.95      0.95      0.95      5856\n\nAUC-ROC: 0.9409\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"strict_dataset, strict_loader, _, _ = load_train_test(\n    f\"{DATASET_DIR}/{DATASET_NAME}/BMAD-AD_{DATASET_NAME}_valid.npz\", device=DEVICE, batch_size=100, test_size=0\n)\n\n# Test set evaluation\nevaluate_model(final_detector, strict_loader, DEVICE, threshold=0.3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:50:50.951499Z","iopub.execute_input":"2026-01-14T08:50:50.951878Z","iopub.status.idle":"2026-01-14T08:50:58.180966Z","shell.execute_reply.started":"2026-01-14T08:50:50.951848Z","shell.execute_reply":"2026-01-14T08:50:58.180322Z"}},"outputs":[{"name":"stdout","text":"Classification Report:\n              precision    recall  f1-score   support\n\n     Class 0       0.95      0.96      0.96      1242\n     Class 1       0.79      0.73      0.76       223\n\n    accuracy                           0.93      1465\n   macro avg       0.87      0.85      0.86      1465\nweighted avg       0.93      0.93      0.93      1465\n\nAUC-ROC: 0.8981\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# from torchvision import transforms\n# from torch.utils.data import TensorDataset\n\n# def preview_images(loader, model=None, device='cpu', num_images=5):\n#     for images, labels in loader:\n#         images = images[:num_images]\n#         labels = labels[:num_images]\n\n#         if model is None:\n#             # Chỉ hiển thị ảnh\n#             imgs_np = images.cpu().numpy()\n#             labels_np = labels.cpu().numpy()\n#             imgs_np = np.clip(imgs_np, 0, 1)\n#             n = imgs_np.shape[0]\n#             fig, axes = plt.subplots(1, n, figsize=(3*n, 3))\n#             if n == 1:\n#                 axes = [axes]\n#             for j in range(n):\n#                 label = int(labels_np[j])\n#                 axes[j].imshow(imgs_np[j].transpose(1, 2, 0))\n#                 axes[j].set_title(f'[{label}] Image {j}')\n#                 axes[j].axis('off')\n#             plt.tight_layout()\n#             plt.show()\n#         else:\n#             # Hiển thị Original vs Reconstructed\n#             model.eval()\n#             with torch.no_grad():\n#                 images = images.to(device)\n#                 labels = labels.to(device)\n#                 reconstructed, _, _ = model(images, labels.unsqueeze(1))\n\n#             orig = images.cpu().numpy()\n#             recon = reconstructed.cpu().numpy()\n#             labels_np = labels.cpu().numpy()\n#             orig = np.clip(orig, 0, 1)\n#             recon = np.clip(recon, 0, 1)\n\n#             n = orig.shape[0]\n#             fig, axes = plt.subplots(2, n, figsize=(4*n, 8))\n#             if n == 1:\n#                 axes = axes.reshape(2, 1)\n\n#             for j in range(n):\n#                 label = int(labels_np[j])\n#                 axes[0, j].imshow(orig[j].transpose(1, 2, 0))\n#                 axes[0, j].set_title(f'[{label}] Org-{j}')\n#                 axes[0, j].axis('off')\n\n#                 axes[1, j].imshow(recon[j].transpose(1, 2, 0))\n#                 axes[1, j].set_title(f'[{label}] Recon-{j}')\n#                 axes[1, j].axis('off')\n#             plt.tight_layout()\n#             plt.show()\n\n#         break  # chỉ lấy 1 batch\n\n# def load_data(data, batch_num=64, is_shuffle=True):\n#     images = data['images']  # shape: (N, H, W, C)\n#     labels = data['labels']\n\n#     # Define transform\n#     transform = transforms.Compose([\n#         transforms.ToPILImage(),            # Chuyển numpy array -> PIL Image\n#         transforms.Resize((256, 256)),      # Resize về 256x256\n#         transforms.ToTensor()               # Chuyển PIL Image -> Tensor [0,1]\n#     ])\n\n#     # Áp dụng transform cho từng ảnh\n#     X = torch.stack([transform(img) for img in images])\n\n#     # Chuyển labels sang tensor\n#     y = torch.tensor(labels, dtype=torch.float32)\n\n#     # Tạo dataset & dataloader\n#     dataset = TensorDataset(X, y)\n#     loader = DataLoader(dataset, batch_size=batch_num, shuffle=is_shuffle)\n\n#     return loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:50:58.181913Z","iopub.execute_input":"2026-01-14T08:50:58.182188Z","iopub.status.idle":"2026-01-14T08:50:58.186725Z","shell.execute_reply.started":"2026-01-14T08:50:58.182164Z","shell.execute_reply":"2026-01-14T08:50:58.185984Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# syn_data = np.load(f\"/kaggle/working/synthetics/syn_ep{len(os.listdir(\"/kaggle/working/synthetics\"))}.npz\", allow_pickle=True)\n# syn_loader = load_data(syn_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:50:58.188542Z","iopub.execute_input":"2026-01-14T08:50:58.189209Z","iopub.status.idle":"2026-01-14T08:50:58.200682Z","shell.execute_reply.started":"2026-01-14T08:50:58.189185Z","shell.execute_reply":"2026-01-14T08:50:58.199993Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# preview_images(syn_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:50:58.201647Z","iopub.execute_input":"2026-01-14T08:50:58.201930Z","iopub.status.idle":"2026-01-14T08:50:58.210776Z","shell.execute_reply.started":"2026-01-14T08:50:58.201904Z","shell.execute_reply":"2026-01-14T08:50:58.210205Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# t_data = np.load(f\"{DATASET_DIR}/{DATASET_NAME}/BMAD-AD_{DATASET_NAME}_valid.npz\", allow_pickle=True)\n# t_loader = load_data(t_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:50:58.211542Z","iopub.execute_input":"2026-01-14T08:50:58.211769Z","iopub.status.idle":"2026-01-14T08:50:58.221650Z","shell.execute_reply.started":"2026-01-14T08:50:58.211748Z","shell.execute_reply":"2026-01-14T08:50:58.221025Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# preview_images(t_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T08:50:58.222382Z","iopub.execute_input":"2026-01-14T08:50:58.222573Z","iopub.status.idle":"2026-01-14T08:50:58.230634Z","shell.execute_reply.started":"2026-01-14T08:50:58.222554Z","shell.execute_reply":"2026-01-14T08:50:58.230001Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}