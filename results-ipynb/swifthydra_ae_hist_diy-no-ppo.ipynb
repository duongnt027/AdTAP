{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LOUzekkZRdL",
        "outputId": "4116b147-f8e3-4483-c743-6b05c2d7348f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pBx79eJlZEmS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from torchvision import models\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DUNwxxTGZEmT"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed=42):\n",
        "    # Python\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "\n",
        "    # NumPy\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # PyTorch\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(11)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XPdlLy5wZEmW"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# ENV\n",
        "# =========================================================\n",
        "class LatentSpaceEnv:\n",
        "    def __init__(self, device=\"cpu\", max_steps=10, target_sim=0.7):\n",
        "        self.device = device\n",
        "        self.max_steps = max_steps\n",
        "        self.target_sim = target_sim\n",
        "\n",
        "        self.X = None\n",
        "        self.Y = None\n",
        "\n",
        "    # ================= DATA =================\n",
        "    def set_data(self, X, Y):\n",
        "        self.X = X.to(self.device)\n",
        "        self.Y = Y.to(self.device)\n",
        "\n",
        "        self.pos_idx = (self.Y == 1).nonzero(as_tuple=True)[0]\n",
        "        self.neg_idx = (self.Y == 0).nonzero(as_tuple=True)[0]\n",
        "\n",
        "        assert len(self.pos_idx) > 0, \"No label=1 samples\"\n",
        "\n",
        "    # ================= RESET =================\n",
        "    def reset(self):\n",
        "        self.current_idx = self.pos_idx[\n",
        "            torch.randint(len(self.pos_idx), (1,))\n",
        "        ].item()\n",
        "\n",
        "        self.x_orig = self.X[self.current_idx]\n",
        "        self.x_current = self.x_orig.clone()\n",
        "\n",
        "        self.step_count = 0\n",
        "        return self.get_state()\n",
        "\n",
        "    # ================= STATE =================\n",
        "    def get_state(self):\n",
        "        return self.x_current.clone()\n",
        "\n",
        "    # ================= REWARD =================\n",
        "    def compute_reward(self, x_prime):\n",
        "        pos_samples = self.X[self.pos_idx]\n",
        "        neg_samples = self.X[self.neg_idx]\n",
        "\n",
        "        mean_pos = torch.mean(torch.norm(x_prime - pos_samples, dim=1))\n",
        "        mean_neg = torch.mean(torch.norm(x_prime - neg_samples, dim=1))\n",
        "        class_margin = torch.sigmoid(mean_neg - mean_pos)\n",
        "\n",
        "        cos_pos = F.cosine_similarity(\n",
        "            x_prime.unsqueeze(0), pos_samples, dim=1\n",
        "        )\n",
        "        max_sim = torch.max(cos_pos)\n",
        "        diversity_penalty = torch.relu(max_sim - 0.95)\n",
        "\n",
        "        sim_orig = F.cosine_similarity(x_prime, self.x_orig, dim=0)\n",
        "        target_reward = torch.exp(\n",
        "            -((sim_orig - self.target_sim) ** 2) / 0.02\n",
        "        )\n",
        "\n",
        "        reward = (\n",
        "            0.5 * class_margin\n",
        "            + 0.4 * target_reward\n",
        "            - 0.6 * diversity_penalty\n",
        "        )\n",
        "\n",
        "        return reward  # scalar\n",
        "\n",
        "    # ================= STEP =================\n",
        "    def step(self, action):\n",
        "        self.step_count += 1\n",
        "\n",
        "        x_prime = self.x_current + action\n",
        "        reward = self.compute_reward(x_prime)\n",
        "\n",
        "        self.x_current = x_prime.detach()\n",
        "        done = self.step_count >= self.max_steps\n",
        "\n",
        "        return self.get_state(), reward, done\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# ACTOR / CRITIC\n",
        "# =========================================================\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden=256):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, hidden),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.mu = nn.Linear(hidden, latent_dim)\n",
        "        self.logstd = nn.Parameter(torch.zeros(latent_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.net(x)\n",
        "        std = self.logstd.exp()\n",
        "        return self.mu(h), std\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden=256):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x).squeeze(-1)\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# ROLLOUT BUFFER\n",
        "# =========================================================\n",
        "class RolloutBuffer:\n",
        "    def __init__(self):\n",
        "        self.clear()\n",
        "\n",
        "    def clear(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.logps = []\n",
        "        self.rewards = []\n",
        "        self.values = []\n",
        "        self.dones = []\n",
        "\n",
        "    def add(self, s, a, logp, r, v, d):\n",
        "        self.states.append(s)\n",
        "        self.actions.append(a)\n",
        "        self.logps.append(logp)\n",
        "        self.rewards.append(r)\n",
        "        self.values.append(v)\n",
        "        self.dones.append(d)\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# PPO\n",
        "# =========================================================\n",
        "class PPO:\n",
        "    def __init__(\n",
        "        self,\n",
        "        actor,\n",
        "        critic,\n",
        "        env,\n",
        "        lr=3e-4,\n",
        "        gamma=0.99,\n",
        "        lam=0.95,\n",
        "        clip_eps=0.2,\n",
        "        train_epochs=5,\n",
        "        device=\"cpu\"\n",
        "    ):\n",
        "        self.actor = actor.to(device)\n",
        "        self.critic = critic.to(device)\n",
        "        self.env = env\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.lam = lam\n",
        "        self.clip_eps = clip_eps\n",
        "        self.train_epochs = train_epochs\n",
        "        self.device = device\n",
        "\n",
        "        self.opt_actor = optim.Adam(self.actor.parameters(), lr=lr)\n",
        "        self.opt_critic = optim.Adam(self.critic.parameters(), lr=lr)\n",
        "\n",
        "        self.buffer = RolloutBuffer()\n",
        "\n",
        "    # ================= ROLLOUT =================\n",
        "    def collect_trajectories(self, num_episodes=4):\n",
        "        self.buffer.clear()\n",
        "        episode_rewards = []\n",
        "\n",
        "        for _ in range(num_episodes):\n",
        "            state = self.env.reset()\n",
        "            ep_reward = 0.0\n",
        "\n",
        "            for _ in range(self.env.max_steps):\n",
        "                state = state.to(self.device)\n",
        "\n",
        "                mu, std = self.actor(state)\n",
        "                dist = torch.distributions.Normal(mu, std)\n",
        "\n",
        "                action = dist.sample()\n",
        "                logp = dist.log_prob(action).sum(dim=-1)\n",
        "\n",
        "                value = self.critic(state)\n",
        "\n",
        "                next_state, reward, done = self.env.step(action)\n",
        "\n",
        "                self.buffer.add(\n",
        "                    state.detach(),\n",
        "                    action.detach(),\n",
        "                    logp.detach(),\n",
        "                    reward.detach(),\n",
        "                    value.detach(),\n",
        "                    done\n",
        "                )\n",
        "\n",
        "                state = next_state\n",
        "                ep_reward += reward.item()\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            episode_rewards.append(ep_reward)\n",
        "\n",
        "        return {\"avg_reward\": float(np.mean(episode_rewards))}\n",
        "\n",
        "    # ================= GAE =================\n",
        "    def compute_gae(self):\n",
        "        rewards = self.buffer.rewards\n",
        "        values = self.buffer.values\n",
        "        dones = self.buffer.dones\n",
        "\n",
        "        advantages = []\n",
        "        returns = []\n",
        "\n",
        "        gae = 0\n",
        "        next_value = 0\n",
        "\n",
        "        for t in reversed(range(len(rewards))):\n",
        "            mask = 1.0 - float(dones[t])\n",
        "            delta = rewards[t] + self.gamma * next_value * mask - values[t]\n",
        "            gae = delta + self.gamma * self.lam * mask * gae\n",
        "\n",
        "            advantages.insert(0, gae)\n",
        "            returns.insert(0, gae + values[t])\n",
        "\n",
        "            next_value = values[t]\n",
        "\n",
        "        return torch.stack(advantages), torch.stack(returns)\n",
        "\n",
        "    # ================= UPDATE =================\n",
        "    def learn(self):\n",
        "        states = torch.stack(self.buffer.states)\n",
        "        actions = torch.stack(self.buffer.actions)\n",
        "        old_logps = torch.stack(self.buffer.logps)\n",
        "\n",
        "        advantages, returns = self.compute_gae()\n",
        "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "        for _ in range(self.train_epochs):\n",
        "            mu, std = self.actor(states)\n",
        "            dist = torch.distributions.Normal(mu, std)\n",
        "            logps = dist.log_prob(actions).sum(dim=-1)\n",
        "\n",
        "            ratio = torch.exp(logps - old_logps)\n",
        "\n",
        "            surr1 = ratio * advantages\n",
        "            surr2 = torch.clamp(\n",
        "                ratio, 1 - self.clip_eps, 1 + self.clip_eps\n",
        "            ) * advantages\n",
        "\n",
        "            actor_loss = -torch.min(surr1, surr2).mean()\n",
        "\n",
        "            values = self.critic(states)\n",
        "            critic_loss = F.mse_loss(values, returns)\n",
        "\n",
        "            self.opt_actor.zero_grad()\n",
        "            actor_loss.backward()\n",
        "            self.opt_actor.step()\n",
        "\n",
        "            self.opt_critic.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            self.opt_critic.step()\n",
        "\n",
        "        avg_reward = torch.stack(self.buffer.rewards).mean().item()\n",
        "        return actor_loss.item(), critic_loss.item(), avg_reward\n",
        "\n",
        "    # ================= INFER =================\n",
        "    @torch.no_grad()\n",
        "    def infer(self, z0, steps=None, step_scale=0.1):\n",
        "        self.actor.eval()\n",
        "\n",
        "        if steps is None:\n",
        "            steps = self.env.max_steps\n",
        "\n",
        "        z = z0.clone().to(self.device)\n",
        "\n",
        "        for _ in range(steps):\n",
        "            mu, _ = self.actor(z)\n",
        "            z = z + step_scale * torch.tanh(mu)\n",
        "\n",
        "        return z\n",
        "\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# OFFLINE REFINEMENT\n",
        "# =========================================================\n",
        "@torch.no_grad()\n",
        "def refine_latent_offline(actor, x0, steps=8, step_scale=0.1):\n",
        "    x = x0.clone()\n",
        "    for _ in range(steps):\n",
        "        mu, _ = actor(x)\n",
        "        x = x + step_scale * torch.tanh(mu)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6Ex_5KX4ZEmW"
      },
      "outputs": [],
      "source": [
        "class NPZDataset(Dataset):\n",
        "    def __init__(self, path, indices=None, resize=256, device=\"cpu\"):\n",
        "        self.data = np.load(path, allow_pickle=True, mmap_mode=\"r\")\n",
        "        self.images = self.data[\"images\"]\n",
        "        self.labels = self.data[\"labels\"]\n",
        "\n",
        "        if indices is None:\n",
        "            self.indices = np.arange(len(self.labels))\n",
        "        else:\n",
        "            self.indices = indices\n",
        "\n",
        "        self.resize = resize\n",
        "        self.device = device\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        i = self.indices[idx]\n",
        "        img = torch.tensor(self.images[i]).float()\n",
        "        lbl = torch.tensor(self.labels[i]).float()\n",
        "\n",
        "        img = img.permute(2,0,1)\n",
        "        if img.max() > 1:\n",
        "            img = img / 255.\n",
        "\n",
        "        img = F.interpolate(\n",
        "            img.unsqueeze(0),\n",
        "            size=(self.resize, self.resize),\n",
        "            mode=\"bilinear\",\n",
        "            align_corners=False\n",
        "        ).squeeze(0)\n",
        "\n",
        "        return img.to(self.device), lbl.to(self.device)\n",
        "\n",
        "class SyntheticDataset(Dataset):\n",
        "    def __init__(self, path, device):\n",
        "        self.data = np.load(path, mmap_mode=\"r\")\n",
        "        self.images = self.data[\"images\"]\n",
        "        self.labels = self.data[\"labels\"]\n",
        "        self.device = device\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = torch.from_numpy(self.images[idx]).float().permute(2,0,1)\n",
        "        lbl = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
        "        return img.to(self.device), lbl.to(self.device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "E63pb19xZEmX"
      },
      "outputs": [],
      "source": [
        "def load_train_test(path, device=\"cpu\", batch_size=32, resize=256, test_size=0.6):\n",
        "    dataset_full = NPZDataset(path, device=device, resize=resize)\n",
        "    indices = np.arange(len(dataset_full.labels))\n",
        "\n",
        "    # Nếu test_size = 0 → không split\n",
        "    if test_size == 0:\n",
        "        train_idx = indices\n",
        "        test_idx = []\n",
        "    else:\n",
        "        train_idx, test_idx = train_test_split(\n",
        "            indices,\n",
        "            test_size=test_size,\n",
        "            stratify=dataset_full.labels,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "    train_dataset = NPZDataset(path, indices=train_idx, device=device, resize=resize)\n",
        "    test_dataset  = NPZDataset(path, indices=test_idx,  device=device, resize=resize) if len(test_idx) > 0 else None\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader  = DataLoader(test_dataset,  batch_size=batch_size) if test_dataset is not None else None\n",
        "\n",
        "    return train_dataset, train_loader, test_dataset, test_loader\n",
        "\n",
        "def count_classes(dataset):\n",
        "    count0 = 0\n",
        "    count1 = 0\n",
        "\n",
        "    for i in range(len(dataset)):\n",
        "        lbl = dataset[i][1]    # dataset[i] → (image, label)\n",
        "        lbl = int(lbl.item())  # convert to 0 hoặc 1\n",
        "\n",
        "        if lbl == 0:\n",
        "            count0 += 1\n",
        "        else:\n",
        "            count1 += 1\n",
        "    return count0, count1\n",
        "\n",
        "def evaluate_model(model, loader, device, threshold=0.5):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in loader:\n",
        "            x_batch = x_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            y_pred = model(x_batch).squeeze()\n",
        "            all_preds.append(y_pred.cpu())\n",
        "            all_labels.append(y_batch.cpu())\n",
        "\n",
        "    preds = torch.cat(all_preds).numpy()\n",
        "    labels = torch.cat(all_labels).numpy()\n",
        "\n",
        "\n",
        "    binary_pred = (preds > threshold).astype(int)\n",
        "\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(labels, binary_pred, labels=[0,1], target_names=[\"Class 0\",\"Class 1\"]))\n",
        "\n",
        "    if len(set(labels)) > 1:\n",
        "        auc = roc_auc_score(labels, preds)\n",
        "        print(f\"AUC-ROC: {auc:.4f}\")\n",
        "    else:\n",
        "        print(\"AUC-ROC: Undefined (only one class present)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "z9rmfbPnZEmX"
      },
      "outputs": [],
      "source": [
        "class ResNetEmbedder(nn.Module):\n",
        "    def __init__(self, output_dim=128, pretrained=True):\n",
        "        super().__init__()\n",
        "        self.backbone = models.resnet50(pretrained=pretrained)\n",
        "        self.backbone.fc = nn.Identity()\n",
        "        self.dim_adjuster = nn.Linear(2048, output_dim)\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def forward(self, images):\n",
        "        features = self.backbone(images)\n",
        "        features = features.squeeze()\n",
        "        if features.dim() == 1:\n",
        "            features = features.unsqueeze(0)\n",
        "        features = self.dim_adjuster(features)\n",
        "        return features\n",
        "\n",
        "class CNNAutoEncoder(nn.Module):\n",
        "    def __init__(self, latent_dim=128):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3,64,4,2,1), nn.BatchNorm2d(64), nn.ReLU(),\n",
        "            nn.Conv2d(64,128,4,2,1), nn.BatchNorm2d(128), nn.ReLU(),\n",
        "            nn.Conv2d(128,256,4,2,1), nn.BatchNorm2d(256), nn.ReLU(),\n",
        "            nn.Conv2d(256,512,4,2,1), nn.BatchNorm2d(512), nn.ReLU()\n",
        "        )\n",
        "        self.encoder2latent = nn.Linear(512*16*16+1, latent_dim)\n",
        "        self.latent2decoder = nn.Linear(latent_dim+1, 512*16*16)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(512,256,4,2,1), nn.BatchNorm2d(256), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(256,128,4,2,1), nn.BatchNorm2d(128), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128,64,4,2,1), nn.BatchNorm2d(64), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64,3,4,2,1), nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x, y):\n",
        "        h = self.encoder(x)\n",
        "        flat = h.view(x.size(0), -1)\n",
        "        xy = torch.cat([flat, y], dim=1)\n",
        "        z = self.encoder2latent(xy)\n",
        "        return z\n",
        "\n",
        "    def decode(self, z, y):\n",
        "        zy = torch.cat([z, y], dim=1)\n",
        "        flat = self.latent2decoder(zy)\n",
        "        h = flat.view(z.size(0),512,16,16)\n",
        "        x_recon = self.decoder(h)\n",
        "        return x_recon\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        z = self.encode(x, y)\n",
        "        return self.decode(z, y)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0,max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0,d_model,2).float()*(-np.log(10000.0)/d_model))\n",
        "        pe[:,0::2] = torch.sin(position*div_term)\n",
        "        pe[:,1::2] = torch.cos(position*div_term)\n",
        "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self,x):\n",
        "        L = x.size(1)\n",
        "        return x + self.pe[:,:L,:]\n",
        "\n",
        "class ViTransformerDetector(nn.Module):\n",
        "    def __init__(self, embedder, d_model=128, nhead=8, num_layers=2, dim_feedforward=256, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedder = embedder\n",
        "        self.embedding_dim = embedder.output_dim\n",
        "        self.projection = nn.Linear(self.embedding_dim,d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,\n",
        "                                                   dim_feedforward=dim_feedforward,\n",
        "                                                   dropout=dropout, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(d_model,128), nn.ReLU(), nn.Dropout(dropout),\n",
        "            nn.Linear(128,1), nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        emb = self.embedder(x).view(x.size(0),1,-1)\n",
        "        emb = self.projection(emb)\n",
        "        emb = self.positional_encoding(emb)\n",
        "        h = self.transformer_encoder(emb)\n",
        "        pooled = h.mean(dim=1)\n",
        "        return self.fc(pooled).squeeze(1)\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=1e-4):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = float(\"inf\")\n",
        "        self.early_stop = False\n",
        "\n",
        "    def step(self, current_loss):\n",
        "        if current_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = current_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "    def reset(self):\n",
        "        self.counter = 0\n",
        "        self.best_loss = float(\"inf\")\n",
        "        self.early_stop = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "S5Fm-8d45mFC"
      },
      "outputs": [],
      "source": [
        "def One_Step_To_Feasible_Action(generator, detector, x_orig, device,\n",
        "                                previously_generated=None, alpha=1.0,\n",
        "                                lambda_div=0.1, lr=0.001, steps=50):\n",
        "    generator.eval()\n",
        "    detector.eval()\n",
        "    if previously_generated is None:\n",
        "        previously_generated = []\n",
        "\n",
        "    x_orig = x_orig.to(device).unsqueeze(0)\n",
        "    y_class1 = torch.full((1,1),0.8,device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        z = generator.encode(x_orig, y_class1)\n",
        "    z = z.clone().detach().requires_grad_(True)\n",
        "\n",
        "    optimizer_z = torch.optim.Adam([z], lr=lr)\n",
        "\n",
        "    for step in range(steps):\n",
        "        optimizer_z.zero_grad()\n",
        "        x_syn = generator.decode(z, y_class1)\n",
        "        prob_class1 = detector(x_syn)\n",
        "\n",
        "        if len(previously_generated) > 0:\n",
        "            old = torch.stack(previously_generated).to(device)\n",
        "            diff = x_syn.unsqueeze(1)-old.unsqueeze(0)\n",
        "            dist_min = diff.norm(p=2,dim=(2,3,4)).min(dim=1).values\n",
        "            diversity_term = torch.exp(-alpha*dist_min).mean()\n",
        "        else:\n",
        "            diversity_term = torch.tensor(0.0, device=device)\n",
        "\n",
        "        reward = prob_class1.mean() + lambda_div*diversity_term\n",
        "        reward.backward()\n",
        "        optimizer_z.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        x_adv = generator.decode(z, y_class1).detach().cpu()\n",
        "\n",
        "    return x_adv\n",
        "\n",
        "def Gen_with_PPO(\n",
        "    actor,\n",
        "    generator,\n",
        "    x_orig,\n",
        "    device,\n",
        "    episodes,\n",
        "    steps=8\n",
        "):\n",
        "    actor.eval()\n",
        "    generator.eval()\n",
        "\n",
        "    # ===== y_class1 GIỮ NGUYÊN =====\n",
        "    conf = 0.8 + 0.1 / (1 + math.exp(-0.1 * (episodes - 10)))\n",
        "    y_class1 = torch.full(\n",
        "        (1, 1),\n",
        "        random.uniform(max(conf - 0.02, 0.8), conf),\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        z0 = generator.encode(\n",
        "            x_orig.unsqueeze(0).to(device),\n",
        "            y_class1\n",
        "        )\n",
        "\n",
        "    # ✅ ĐÚNG TÊN HÀM\n",
        "    z_adv = refine_latent_offline(\n",
        "        actor,\n",
        "        z0.squeeze(0),\n",
        "        steps=steps\n",
        "    )\n",
        "\n",
        "    if not check_valid_variant(z_adv, z0.squeeze(0)):\n",
        "        return None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        x_synthetic = generator.decode(\n",
        "            z_adv.unsqueeze(0),\n",
        "            y_class1\n",
        "        )\n",
        "\n",
        "    return x_synthetic\n",
        "\n",
        "\n",
        "def check_valid_variant(x, x_prime, threshold=0.3):\n",
        "    # Flatten nếu có batch dimension\n",
        "    if x.dim() > 1:\n",
        "        x_flat = x.view(x.size(0), -1)\n",
        "        x_prime_flat = x_prime.view(x_prime.size(0), -1)\n",
        "        cos_sim = torch.nn.functional.cosine_similarity(x_flat, x_prime_flat, dim=1)\n",
        "    else:\n",
        "        cos_sim = torch.nn.functional.cosine_similarity(x, x_prime, dim=0)\n",
        "\n",
        "    cos_dist = 1 - cos_sim\n",
        "\n",
        "    # Trả về boolean value duy nhất\n",
        "    # Nếu có nhiều samples, kiểm tra xem TẤT CẢ có valid không\n",
        "    is_valid = (cos_dist < threshold).all()\n",
        "\n",
        "    return is_valid.item()  # Convert tensor to Python bool\n",
        "\n",
        "def load_z_space(generator, loader, device):\n",
        "    generator.eval()\n",
        "\n",
        "    all_z = []\n",
        "    all_y = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device)\n",
        "            y = y.unsqueeze(1).to(device)\n",
        "\n",
        "            z = generator.encode(x, y)   # (batch, z_dim)\n",
        "\n",
        "            all_z.append(z.cpu())\n",
        "            all_y.append(y.cpu())\n",
        "\n",
        "    # Ghép toàn bộ batch lại\n",
        "    all_z = torch.cat(all_z, dim=0)\n",
        "    all_y = torch.cat(all_y, dim=0)\n",
        "\n",
        "    return all_z, all_y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UykE1uCsZEmX"
      },
      "outputs": [],
      "source": [
        "def train_cnn_ae(model, loader, opt, device):\n",
        "    model.train()\n",
        "    total = 0\n",
        "    for x,y in loader:\n",
        "        x = x.to(device)\n",
        "        y = y.unsqueeze(1).to(device)\n",
        "        recon = model.decode(model.encode(x,y), y)\n",
        "        loss = torch.mean(torch.abs(x - recon))\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total += loss.item()\n",
        "    return total/len(loader)\n",
        "\n",
        "def train_detector(model, loader, opt, criterion, device):\n",
        "    model.train()\n",
        "    total = 0\n",
        "    for x,y in loader:\n",
        "        x=x.to(device); y=y.to(device)\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total += loss.item()\n",
        "    return total/len(loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MiiekXBJZEmY",
        "outputId": "5ff40518-2c63-4f1d-a78b-7ffe0e379eff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 128MB/s]\n"
          ]
        }
      ],
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DATASET_DIR = \"/content/drive/MyDrive/BMAD-AD\"\n",
        "# DATASET_NAME = \"RESC\"\n",
        "# DATASET_NAME = \"BraTS2021_slice\"\n",
        "DATASET_NAME = \"hist_DIY\"\n",
        "DATASET_PATH = f\"{DATASET_DIR}/{DATASET_NAME}/BMAD-AD_{DATASET_NAME}_train.npz\"\n",
        "SYNTHETIC_DIR = f\"synthetics/\"\n",
        "os.makedirs(SYNTHETIC_DIR, exist_ok=True)\n",
        "MODEL_DIR = f\"{DATASET_NAME}_models/\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "PATIENTCE = 5\n",
        "BATCH_SIZE = 100\n",
        "LATENT_DIM = 64\n",
        "REFINE_STEP = 10\n",
        "\n",
        "train_dataset, train_loader, test_dataset, test_loader = load_train_test(\n",
        "    DATASET_PATH, device=DEVICE, batch_size=BATCH_SIZE, resize=256\n",
        ")\n",
        "\n",
        "# # ===== ENV =====\n",
        "# env = LatentSpaceEnv(\n",
        "#     device=DEVICE,\n",
        "#     max_steps=REFINE_STEP\n",
        "# )\n",
        "\n",
        "# # ===== ACTOR / CRITIC =====\n",
        "# actor = Actor(\n",
        "#     latent_dim=LATENT_DIM,\n",
        "#     hidden=256\n",
        "# )\n",
        "\n",
        "# critic = Critic(\n",
        "#     latent_dim=LATENT_DIM,\n",
        "#     hidden=256\n",
        "# )\n",
        "\n",
        "# # ===== PPO =====\n",
        "# ppo = PPO(\n",
        "#     actor=actor,\n",
        "#     critic=critic,\n",
        "#     env=env,\n",
        "#     lr=3e-4,\n",
        "#     gamma=0.99,\n",
        "#     lam=0.95,\n",
        "#     clip_eps=0.2,\n",
        "#     device=DEVICE\n",
        "# )\n",
        "\n",
        "# generator = CNNAutoEncoder(latent_dim=LATENT_DIM).to(DEVICE)\n",
        "embedder = ResNetEmbedder(output_dim=128).to(DEVICE)\n",
        "# detector = ViTransformerDetector(embedder=embedder).to(DEVICE)\n",
        "early_stopper = EarlyStopping(patience=PATIENTCE)\n",
        "\n",
        "# optimizer_g = torch.optim.Adam(generator.parameters(), lr=1e-4)\n",
        "# optimizer_d = torch.optim.Adam(detector.parameters(), lr=1e-4)\n",
        "# criterion = nn.BCELoss()\n",
        "\n",
        "# synthetic_files = []\n",
        "# num_epochs_generator = 100\n",
        "# num_epochs_detector = 100\n",
        "# num_episodes = 100\n",
        "# num_steps_ppo = 100\n",
        "# num_gen_data = 100\n",
        "# combined_dataset = train_dataset\n",
        "\n",
        "# for ep in range(num_episodes):\n",
        "#     print(f\"\\n================ EPISODE {ep+1}/{num_episodes} ================\")\n",
        "#     total_class0, total_class1 = count_classes(combined_dataset)\n",
        "\n",
        "#     # 2️⃣ Print\n",
        "#     print(f\"[Episode {ep+1}] Class count → 0: {total_class0}, 1: {total_class1}\")\n",
        "\n",
        "#     # 3️⃣ Auto-stop if balanced\n",
        "#     if total_class1 >= total_class0:\n",
        "#         print(\"Dataset is now balanced. Stopping synthetic generation.\")\n",
        "#         break\n",
        "\n",
        "#     # TRAIN GENERATOR\n",
        "#     early_stopper.reset()\n",
        "#     for epoch in range(num_epochs_generator):\n",
        "#         loss_g = train_cnn_ae(generator, train_loader, optimizer_g, DEVICE)\n",
        "#         if (epoch + 1) % 10 == 0:\n",
        "#             print(f\"[GENERATOR] Epoch {epoch+1}/{num_epochs_generator}, Loss={loss_g:.4f}\")\n",
        "#         early_stopper.step(loss_g)\n",
        "#         if early_stopper.early_stop:\n",
        "#             print(f\"[GENERATOR] Early stopping at epoch {epoch+1}\")\n",
        "#             print(f\"[GENERATOR] Epoch {epoch+1}/{num_epochs_generator}, Loss={loss_g:.4f}\")\n",
        "#             break\n",
        "\n",
        "#     torch.save(generator.state_dict(), f\"{MODEL_DIR}/{DATASET_NAME}_generator.pth\")\n",
        "\n",
        "#     # 4️⃣ Train detector\n",
        "\n",
        "#     # early_stopper.reset()\n",
        "#     # for epoch in range(num_epochs_detector):\n",
        "#     #     loss_d = train_detector(detector, train_loader, optimizer_d, criterion, DEVICE)\n",
        "#     #     if (epoch + 1) % 10 == 0:\n",
        "#     #         print(f\"[DETECTOR] Epoch {epoch+1}/{num_epochs_detector}, Loss={loss_d:.4f}\")\n",
        "#     #     early_stopper.step(loss_d)\n",
        "#     #     if early_stopper.early_stop:\n",
        "#     #         print(f\"[DETECTOR] Early stopping at epoch {epoch+1}\")\n",
        "#     #         print(f\"[DETECTOR] Epoch {epoch+1}/{num_epochs_detector}, Loss={loss_d:.4f}\")\n",
        "#     #         break\n",
        "\n",
        "#     # torch.save(detector.state_dict(), f\"{MODEL_DIR}/{DATASET_NAME}_detector.pth\")\n",
        "\n",
        "\n",
        "#     # 5️⃣ Generate synthetic samples (class 1)\n",
        "#     synthetic_samples = []\n",
        "#     idx_class1 = [i for i, idx_lbl in enumerate(train_dataset.indices) if train_dataset.labels[idx_lbl] == 1]\n",
        "\n",
        "#     ppo_gen_count = 0\n",
        "#     for _ in range(num_gen_data):\n",
        "#         if not idx_class1:\n",
        "#             break\n",
        "#         rand_idx = np.random.choice(idx_class1)\n",
        "#         x_orig, _ = train_dataset[rand_idx]\n",
        "#         x_adv = None\n",
        "#         if ep != 0:\n",
        "#             x_adv = Gen_with_PPO(actor, generator, x_orig, DEVICE, ep+1, REFINE_STEP)\n",
        "#         if x_adv is None:\n",
        "#             x_adv = One_Step_To_Feasible_Action(generator, detector, x_orig, DEVICE)\n",
        "#         else:\n",
        "#             ppo_gen_count += 1\n",
        "#         synthetic_samples.append(x_adv.to(DEVICE))\n",
        "\n",
        "#     ppo_gen_rate  = (ppo_gen_count / num_gen_data) * 100\n",
        "#     print(f\"{ppo_gen_rate:.2f}% of data generated by PPO strategy\")\n",
        "\n",
        "#     syn_tensor = torch.cat(synthetic_samples)            # (N,3,H,W)\n",
        "#     syn_tensor_hwc = syn_tensor.permute(0,2,3,1).cpu().numpy()   # (N,H,W,3)  <-- HWC\n",
        "#     syn_labels = np.ones(len(synthetic_samples), dtype=np.float32)\n",
        "\n",
        "#     syn_path = f\"{SYNTHETIC_DIR}/syn_ep{ep+1}.npz\"\n",
        "#     np.savez_compressed(syn_path, images=syn_tensor_hwc, labels=syn_labels)\n",
        "#     synthetic_files.append(syn_path)\n",
        "\n",
        "#     print(f\"[EP {ep+1}] Generated synthetic saved:\", syn_path)\n",
        "\n",
        "#     synthetic_subsets = []\n",
        "#     for f in synthetic_files:\n",
        "#         synthetic_subsets.append(SyntheticDataset(f, DEVICE))\n",
        "\n",
        "#     synthetic_full = ConcatDataset(synthetic_subsets)\n",
        "#     combined_dataset = ConcatDataset([train_dataset, synthetic_full])\n",
        "\n",
        "#     train_loader = DataLoader(combined_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "#     Z_ppo, Y_ppo = load_z_space(generator, train_loader, DEVICE)\n",
        "#     # env.set_models(generator, detector)\n",
        "#     env.set_data(Z_ppo, Y_ppo)\n",
        "#     # ===== TRAIN PPO =====\n",
        "#     early_stopper.reset()\n",
        "\n",
        "#     for step in range(num_steps_ppo):\n",
        "\n",
        "#         rollout_stats = ppo.collect_trajectories(\n",
        "#             num_episodes=4   # ✅ FIX CỨNG, KHÔNG DÙNG env.batch_size\n",
        "#         )\n",
        "\n",
        "#         a_loss, c_loss, avg_reward = ppo.learn()\n",
        "\n",
        "#         if (step + 1) % 1 == 0:\n",
        "#             print(\n",
        "#                 f\"Step {step:03d} | \"\n",
        "#                 f\"Actor Loss = {a_loss:.4f}, \"\n",
        "#                 f\"Critic Loss = {c_loss:.4f}, \"\n",
        "#                 f\"AvgReward = {avg_reward:.4f}\"\n",
        "#             )\n",
        "\n",
        "#         early_stopper.step(-avg_reward)\n",
        "#         if early_stopper.early_stop:\n",
        "#             print(f\"[PPO] Early stopping at step {step+1}\")\n",
        "#             print(\n",
        "#                 f\"Step {step:03d} | \"\n",
        "#                 f\"Actor Loss = {a_loss:.4f}, \"\n",
        "#                 f\"Critic Loss = {c_loss:.4f}, \"\n",
        "#                 f\"AvgReward = {avg_reward:.4f}\"\n",
        "#             )\n",
        "#             break\n",
        "\n",
        "\n",
        "#     print(f\"[EP {ep+1}] Train dataset expanded: now {len(combined_dataset)} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WS-gcviBZEmY",
        "outputId": "e24ad330-3685-4047-9133-ec5c4008682e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FINAL DETECTOR] Epoch 1/100, Loss=0.5273\n",
            "[FINAL DETECTOR] Epoch 2/100, Loss=0.4251\n",
            "[FINAL DETECTOR] Epoch 3/100, Loss=0.4064\n",
            "[FINAL DETECTOR] Epoch 4/100, Loss=0.2949\n",
            "[FINAL DETECTOR] Epoch 5/100, Loss=0.2632\n",
            "[FINAL DETECTOR] Epoch 6/100, Loss=0.1845\n",
            "[FINAL DETECTOR] Epoch 7/100, Loss=0.1967\n",
            "[FINAL DETECTOR] Epoch 8/100, Loss=0.2350\n",
            "[FINAL DETECTOR] Epoch 9/100, Loss=0.2349\n",
            "[FINAL DETECTOR] Epoch 10/100, Loss=0.1900\n",
            "[FINAL DETECTOR] Epoch 11/100, Loss=0.1262\n",
            "[FINAL DETECTOR] Epoch 12/100, Loss=0.1732\n",
            "[FINAL DETECTOR] Epoch 13/100, Loss=0.1881\n",
            "[FINAL DETECTOR] Epoch 14/100, Loss=0.1046\n",
            "[FINAL DETECTOR] Epoch 15/100, Loss=0.0790\n",
            "[FINAL DETECTOR] Epoch 16/100, Loss=0.0969\n",
            "[FINAL DETECTOR] Epoch 17/100, Loss=0.0801\n",
            "[FINAL DETECTOR] Epoch 18/100, Loss=0.0713\n",
            "[FINAL DETECTOR] Epoch 19/100, Loss=0.0890\n",
            "[FINAL DETECTOR] Epoch 20/100, Loss=0.0464\n",
            "[FINAL DETECTOR] Epoch 21/100, Loss=0.0540\n",
            "[FINAL DETECTOR] Epoch 22/100, Loss=0.0541\n",
            "[FINAL DETECTOR] Epoch 23/100, Loss=0.0494\n",
            "[FINAL DETECTOR] Epoch 24/100, Loss=0.0417\n",
            "[FINAL DETECTOR] Epoch 25/100, Loss=0.0373\n",
            "[FINAL DETECTOR] Epoch 26/100, Loss=0.0343\n",
            "[FINAL DETECTOR] Epoch 27/100, Loss=0.0390\n",
            "[FINAL DETECTOR] Epoch 28/100, Loss=0.0287\n",
            "[FINAL DETECTOR] Epoch 29/100, Loss=0.0309\n",
            "[FINAL DETECTOR] Epoch 30/100, Loss=0.0156\n",
            "[FINAL DETECTOR] Epoch 31/100, Loss=0.0092\n",
            "[FINAL DETECTOR] Epoch 32/100, Loss=0.0145\n",
            "[FINAL DETECTOR] Epoch 33/100, Loss=0.0284\n",
            "[FINAL DETECTOR] Epoch 34/100, Loss=0.0114\n",
            "[FINAL DETECTOR] Epoch 35/100, Loss=0.0022\n",
            "[FINAL DETECTOR] Epoch 36/100, Loss=0.0048\n",
            "[FINAL DETECTOR] Epoch 37/100, Loss=0.0160\n",
            "[FINAL DETECTOR] Epoch 38/100, Loss=0.0765\n",
            "[FINAL DETECTOR] Epoch 39/100, Loss=0.0313\n",
            "[FINAL DETECTOR] Epoch 40/100, Loss=0.0683\n",
            "[FINAL DETECTOR] Early stopping at epoch 40\n",
            "[FINAL DETECTOR] Epoch 40/100, Loss=0.0683\n"
          ]
        }
      ],
      "source": [
        "final_detector = ViTransformerDetector(embedder=embedder).to(DEVICE)\n",
        "optimizer_final = torch.optim.Adam(final_detector.parameters(), lr=1e-3)\n",
        "criterion = nn.BCELoss()\n",
        "num_epochs_final = 100\n",
        "early_stopper.reset()\n",
        "\n",
        "for epoch in range(num_epochs_final):\n",
        "    loss = train_detector(final_detector, train_loader, optimizer_final, criterion, DEVICE)\n",
        "    print(f\"[FINAL DETECTOR] Epoch {epoch+1}/{num_epochs_final}, Loss={loss:.4f}\")\n",
        "\n",
        "    # Check early stopping\n",
        "    early_stopper.step(loss)\n",
        "\n",
        "    if early_stopper.early_stop:\n",
        "        print(f\"[FINAL DETECTOR] Early stopping at epoch {epoch+1}\")\n",
        "        print(f\"[FINAL DETECTOR] Epoch {epoch+1}/{num_epochs_final}, Loss={loss:.4f}\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1tyfYnFJZEmZ",
        "outputId": "8d222c76-5ce3-426c-def5-2e602e4dedaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.94      1.00      0.97      1184\n",
            "     Class 1       0.99      0.77      0.87       352\n",
            "\n",
            "    accuracy                           0.95      1536\n",
            "   macro avg       0.96      0.89      0.92      1536\n",
            "weighted avg       0.95      0.95      0.94      1536\n",
            "\n",
            "AUC-ROC: 0.9247\n",
            "Final detector saved to hist_DIY_models//hist_DIY_detector_final.pth\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(final_detector, test_loader, DEVICE, threshold=0.3)\n",
        "\n",
        "# Save final detector\n",
        "torch.save(final_detector.state_dict(), f\"{MODEL_DIR}/{DATASET_NAME}_detector_final.pth\")\n",
        "print(f\"Final detector saved to {MODEL_DIR}/{DATASET_NAME}_detector_final.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "PxWs43QiZEmZ"
      },
      "outputs": [],
      "source": [
        "# torch.save(actor.state_dict(), f\"{MODEL_DIR}/{DATASET_NAME}_actor.pth\")\n",
        "# print(f\"PPO Actor saved to {MODEL_DIR}/{DATASET_NAME}_actor.pth\")\n",
        "\n",
        "# torch.save(critic.state_dict(), f\"{MODEL_DIR}/{DATASET_NAME}_critic.pth\")\n",
        "# print(f\"PPO Critic saved to {MODEL_DIR}/{DATASET_NAME}_critic.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "s-J4CU-65rB0"
      },
      "outputs": [],
      "source": [
        "# import shutil\n",
        "\n",
        "# synthetics_folder = \"/content/synthetics\"\n",
        "# synthetics_zip = f\"{DATASET_NAME}_synthetics\"\n",
        "\n",
        "# shutil.make_archive(synthetics_zip, 'zip', synthetics_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "bjVo6BwyZEmZ",
        "outputId": "bf639458-59a2-4316-e6c7-1ae6f1c03582"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/hist_DIY_models.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "models_folder = f\"/content/{DATASET_NAME}_models\"\n",
        "models_zip = f\"{DATASET_NAME}_models\"\n",
        "\n",
        "shutil.make_archive(models_zip, 'zip', models_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JR7TSsV-ZEmZ",
        "outputId": "44581d14-cc19-461a-a2e2-3486f5ded7ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.95      1.00      0.97      1974\n",
            "     Class 1       0.99      0.81      0.89       586\n",
            "\n",
            "    accuracy                           0.95      2560\n",
            "   macro avg       0.97      0.90      0.93      2560\n",
            "weighted avg       0.96      0.95      0.95      2560\n",
            "\n",
            "AUC-ROC: 0.9407\n"
          ]
        }
      ],
      "source": [
        "strict_dataset, strict_loader, _, _ = load_train_test(\n",
        "    f\"{DATASET_DIR}/{DATASET_NAME}/BMAD-AD_{DATASET_NAME}_train.npz\", device=DEVICE, batch_size=100, test_size=0\n",
        ")\n",
        "\n",
        "# Test set evaluation\n",
        "evaluate_model(final_detector, strict_loader, DEVICE, threshold=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a87s4URZEmZ",
        "outputId": "9da91035-0240-4179-8f38-ab08be7a00aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.93      0.99      0.96       494\n",
            "     Class 1       0.97      0.74      0.84       147\n",
            "\n",
            "    accuracy                           0.94       641\n",
            "   macro avg       0.95      0.87      0.90       641\n",
            "weighted avg       0.94      0.94      0.93       641\n",
            "\n",
            "AUC-ROC: 0.8778\n"
          ]
        }
      ],
      "source": [
        "strict_dataset, strict_loader, _, _ = load_train_test(\n",
        "    f\"{DATASET_DIR}/{DATASET_NAME}/BMAD-AD_{DATASET_NAME}_valid.npz\", device=DEVICE, batch_size=100, test_size=0\n",
        ")\n",
        "\n",
        "# Test set evaluation\n",
        "evaluate_model(final_detector, strict_loader, DEVICE, threshold=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "aEamYudHZEma"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# from torchvision import transforms\n",
        "# from torch.utils.data import TensorDataset\n",
        "\n",
        "# def preview_images(loader, model=None, device='cpu', num_images=5):\n",
        "#     for images, labels in loader:\n",
        "#         images = images[:num_images]\n",
        "#         labels = labels[:num_images]\n",
        "\n",
        "#         if model is None:\n",
        "#             # Chỉ hiển thị ảnh\n",
        "#             imgs_np = images.cpu().numpy()\n",
        "#             labels_np = labels.cpu().numpy()\n",
        "#             imgs_np = np.clip(imgs_np, 0, 1)\n",
        "#             n = imgs_np.shape[0]\n",
        "#             fig, axes = plt.subplots(1, n, figsize=(3*n, 3))\n",
        "#             if n == 1:\n",
        "#                 axes = [axes]\n",
        "#             for j in range(n):\n",
        "#                 label = int(labels_np[j])\n",
        "#                 axes[j].imshow(imgs_np[j].transpose(1, 2, 0))\n",
        "#                 axes[j].set_title(f'[{label}] Image {j}')\n",
        "#                 axes[j].axis('off')\n",
        "#             plt.tight_layout()\n",
        "#             plt.show()\n",
        "#         else:\n",
        "#             # Hiển thị Original vs Reconstructed\n",
        "#             model.eval()\n",
        "#             with torch.no_grad():\n",
        "#                 images = images.to(device)\n",
        "#                 labels = labels.to(device)\n",
        "#                 reconstructed, _, _ = model(images, labels.unsqueeze(1))\n",
        "\n",
        "#             orig = images.cpu().numpy()\n",
        "#             recon = reconstructed.cpu().numpy()\n",
        "#             labels_np = labels.cpu().numpy()\n",
        "#             orig = np.clip(orig, 0, 1)\n",
        "#             recon = np.clip(recon, 0, 1)\n",
        "\n",
        "#             n = orig.shape[0]\n",
        "#             fig, axes = plt.subplots(2, n, figsize=(4*n, 8))\n",
        "#             if n == 1:\n",
        "#                 axes = axes.reshape(2, 1)\n",
        "\n",
        "#             for j in range(n):\n",
        "#                 label = int(labels_np[j])\n",
        "#                 axes[0, j].imshow(orig[j].transpose(1, 2, 0))\n",
        "#                 axes[0, j].set_title(f'[{label}] Org-{j}')\n",
        "#                 axes[0, j].axis('off')\n",
        "\n",
        "#                 axes[1, j].imshow(recon[j].transpose(1, 2, 0))\n",
        "#                 axes[1, j].set_title(f'[{label}] Recon-{j}')\n",
        "#                 axes[1, j].axis('off')\n",
        "#             plt.tight_layout()\n",
        "#             plt.show()\n",
        "\n",
        "#         break  # chỉ lấy 1 batch\n",
        "\n",
        "# def load_data(data, batch_num=64, is_shuffle=True):\n",
        "#     images = data['images']  # shape: (N, H, W, C)\n",
        "#     labels = data['labels']\n",
        "\n",
        "#     # Define transform\n",
        "#     transform = transforms.Compose([\n",
        "#         transforms.ToPILImage(),            # Chuyển numpy array -> PIL Image\n",
        "#         transforms.Resize((256, 256)),      # Resize về 256x256\n",
        "#         transforms.ToTensor()               # Chuyển PIL Image -> Tensor [0,1]\n",
        "#     ])\n",
        "\n",
        "#     # Áp dụng transform cho từng ảnh\n",
        "#     X = torch.stack([transform(img) for img in images])\n",
        "\n",
        "#     # Chuyển labels sang tensor\n",
        "#     y = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "#     # Tạo dataset & dataloader\n",
        "#     dataset = TensorDataset(X, y)\n",
        "#     loader = DataLoader(dataset, batch_size=batch_num, shuffle=is_shuffle)\n",
        "\n",
        "#     return loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "A2wEsPiPZEma"
      },
      "outputs": [],
      "source": [
        "# syn_data = np.load(f\"/content/synthetics/syn_ep{len(os.listdir(\"/content/synthetics\"))}.npz\", allow_pickle=True)\n",
        "# syn_loader = load_data(syn_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ZnklGoioZEma"
      },
      "outputs": [],
      "source": [
        "# preview_images(syn_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "0kA51K0cZEma"
      },
      "outputs": [],
      "source": [
        "# t_data = np.load(f\"{DATASET_DIR}/{DATASET_NAME}/BMAD-AD_{DATASET_NAME}_valid.npz\", allow_pickle=True)\n",
        "# t_loader = load_data(t_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "nh2FBHFKZEma"
      },
      "outputs": [],
      "source": [
        "# preview_images(t_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "spKaPSYCEuXh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 8727126,
          "sourceId": 14065279,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31193,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}